---
# *_managed vars denote whether ansible takes care of the application or not.
#   `X_managed: false` application will still fire if it is in `Y_managed: true`
#   application dependencies
#
# *_enabled vars denote whether the application is enabled or not. Unlike
#   X_managed it disables the application when `X_enabled: false`

###
### Server connection configuration
###
# Don't keep passwords in plain text, encrypt them. One option would be:
#   ansible-vault encrypt_string -J PASSWORD_PLACEHOLDER
ansible_user: ansible-guy   # <- Not required for ssh-key-based connection
ansible_password: changeme  # <- Not required for ssh-key-based connection
ansible_become_password: changeme

# All before this ROLES_CONF_TS4LE64m91 marker stays
# unchanged by sample-vars.sh script
#################### {{ ROLES_CONF_TS4LE64m91 }} ####################



########################
###       BASE       ###
########################

# ===============
#       AGE
# ===============
# Versions: https://github.com/FiloSottile/age/releases
# -----
age_managed: false
age_version:    # Leave blank to leverage bookshelf configuration

# ================
#       BASH
# ================
bash_managed: false
bash_completion: false

# ==================
#       DOCKER
# ==================
# docker_users:   # <- Existing users to be added to docker group
#   - "{{ ansible_user_id }}"
docker_managed: false
docker_users:

# =================
#       ENVAR
# =================
# Installs envar for bash
# -----
# envar_users:  # <- Existing users to have access to envar
#   - "{{ ansible_user_id }}"
#   - root
envar_managed: false
envar_users:

# ===============
#       FZF
# ===============
# Versions: https://github.com/junegunn/fzf/releases
# -----
# fzf_users:  # <- Existing users to have access to fzf
#   - "{{ ansible_user_id }}"
#   - root
fzf_managed: false
fzf_version:    # Leave blank to leverage bookshelf configuration
fzf_users:

# ===============
#       GIT
# ===============
# git_extraconf:
#   - owner: "{{ ansible_user_id }}"    # <- Required, must be unique
#     template: gitconfig.extra.ini.j2  # <- Required. No full path for the ones defined in the role
#     # Optional variables, can be addressed in the template via 'conf' map
#     user_name: Foo            # <- 'conf.user_name' in the template
#     user_email: foo@bar.baz
#   - owner: another-user
#     template: "{{ playbook_dir }}/resources/git/extra.ini.j2" # <- Full path for ones out of the role directory
#     # ...
git_managed: false
git_extraconf:

# ===================
#       PS1-GIT
# ===================
# Installs fancy PS1 with git PS1 suffix support. Loaded with envar.
# Doesn't require git
# -----
# ps1_git_users:  # <- Existing users to have this PS1
#   - "{{ ansible_user_id }}"
#   - root
ps1_git_managed: false
ps1_git_users:

# =================
#       SNAPD
# =================
# Not supported by Alpine
# -----
# snapd_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines
# -----
snapd_managed: false

# ============================
#       TAILSCALE-CLIENT
# ============================
tailscale_client_managed: false

# ================
#       TMUX
# ================
# tmux_sane_users:   # <- Existing users to source my preferred configuration
#   - "{{ ansible_user_id }}"
tmux_managed: false
tmux_sane_users:

# =================
#       TMUXP
# =================
# Not supported by Alpine
# -----
# tmuxp_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines
# -----
# tmuxp_conf:
#   - owner: "{{ ansible_user_id }}"
#     conf:
#         # Also available: triple.yaml.j2, quad.yaml.j2
#       - template: sensors.yaml.j2   # <- Required. No full path for the ones defined in the role
#         name: my-sensors            # <- Required. Will land in ~/.tmuxp/my-sensors.yaml
#         # Optional variables, can be addressed in the template via 'conf' map
#         session_name: Foo           # <- 'conf.session_name' in the template
#         window_name: bar
#       - template: "{{ playbook_dir }}/resources/tmuxp/demo1.yaml.j2"  # <- Full path for ones out of the role directory
#         ...
#   - owner: ...
#     ...
tmuxp_managed: false
tmuxp_conf:

# ====================
#       TOOLBELT
# ====================
# Contains very basic and often needed tools that for most often usage don't
# require any additional configuration. Some of the tools are categorized and
# the categories can be used to install the whole set of underlying tools.
# -----
# * editors:      nano, neovim, vim
# * downloaders:  curl, wget
# * htop, jq, sensors, speedtest, tar
# -----
# toolbelt_pick:    # <- Install only a fraction. Leave empty to install all
#   - nano            # <- curl will be installed
#   - downloaders     # <- All downloaders will be installed
#   - tar             # <- tar will be installed
# toolbelt_exclude: # <- Opposed to toolbelt_pick, higher priority than toolbelt_pick
#   - wget            # <- In conjunction with toolbelt_pick above, only curl will be installed
#   - editors         # <- Dispite toolbelt_pick above, nano will not be installed
toolbelt_managed: false
toolbelt_pick:
toolbelt_exclude:



###########################
###       SERVICE       ###
###########################

# ===================
#       ADGUARD
# ===================
# Versions:
# * https://hub.docker.com/r/adguard/adguardhome/tags
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
adguard_managed: false
adguard_enabled: false
adguard_version:        # Leave blank to leverage bookshelf configuration
adguard_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
adguard_owner: "{{ ansible_user_id }}"
adguard_conf_dir: ~/conf/adguard
adguard_service_name: adguard
adguard_hostname: "{{ adguard_service_name }}"
adguard_vhost: "{{ adguard_service_name }}.domain.local"
adguard_web_ui_port:    # Leave blank to leverage bookshelf configuration
adguard_init_ui_port:   # Leave blank to leverage bookshelf configuration
adguard_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file, will affect both adguard
# and tailscale. Ex.: "{{ playbook_dir }}/resources/adguard/extra.env.j2"
adguard_extra_envfile:
# ----------
# The following settings configure Adguard to run as a part of tailnet. This
# allows to configure adguard for a tailnet DNS server under
# https://tailscale.com/admin/dns -> Nameservers
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
#
# NOTE 1: Local network is not accessible within tailnet, which means that if i
# create an adguard DNS rewrite for 'my-site.local' to '192.168.0.20', the site
# will not be reachable in the tailnet. SOLUTION: deploy another tailscale
# service with "advertised routes" (https://tailscale.com/kb/1019/subnets)
adguard_tailscaled: false
adguard_tailscale_version:    # Leave blank to leverage bookshelf configuration
adguard_tailscale_service_name: "{{ adguard_service_name }}-tailscale"
# Hostname within tailnet
adguard_tailscale_ts_hostname: "{{ adguard_tailscale_service_name }}"
adguard_tailscale_auth_key:

# ==================
#       GOTIFY
# ==================
# Versions:
# * https://hub.docker.com/r/gotify/server/tags
# -----
gotify_managed: false
gotify_enabled: false
gotify_version:       # Leave blank to leverage bookshelf configuration
gotify_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
gotify_owner: "{{ ansible_user_id }}"
gotify_conf_dir: ~/conf/gotify
gotify_service_name: gotify
gotify_hostname: "{{ gotify_service_name }}"
gotify_vhost: "{{ gotify_service_name }}.domain.local"
gotify_web_ui_port:   # Leave blank to leverage bookshelf configuration
gotify_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# ====================
#       HEIMDALL
# ====================
# Versions:
# * https://hub.docker.com/r/linuxserver/heimdall/tags
# -----
heimdall_managed: false
heimdall_enabled: false
heimdall_version:       # Leave blank to leverage bookshelf configuration
heimdall_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
heimdall_owner: "{{ ansible_user_id }}"
heimdall_conf_dir: ~/conf/heimdall
heimdall_service_name: heimdall
heimdall_hostname: "{{ heimdall_service_name }}"
heimdall_vhost: "{{ heimdall_service_name }}.domain.local"
heimdall_web_ui_port:   # Leave blank to leverage bookshelf configuration
heimdall_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# ====================
#       JELLYFIN
# ====================
# Versions:
# * https://hub.docker.com/r/linuxserver/jellyfin/tags
# -----
jellyfin_managed: false
jellyfin_enabled: false
jellyfin_version:       # Leave blank to leverage bookshelf configuration
jellyfin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
jellyfin_owner: "{{ ansible_user_id }}"
jellyfin_conf_dir: ~/conf/jellyfin
jellyfin_data_dir:      # Directory with media files. Required
jellyfin_service_name: jellyfin
jellyfin_hostname: "{{ jellyfin_service_name }}"
jellyfin_vhost: "{{ jellyfin_service_name }}.domain.local"
jellyfin_web_ui_port:   # Leave blank to leverage bookshelf configuration
jellyfin_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/jellyfin/extra.env.j2"
jellyfin_extra_envfile:
# ----------
# The following settings configure Jellyfin to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jellyfin_tailscaled: false
jellyfin_tailscale_version:    # Leave blank to leverage bookshelf configuration
jellyfin_tailscale_service_name: "{{ jellyfin_service_name }}-tailscale"
# Hostname within tailnet
jellyfin_tailscale_ts_hostname: "{{ jellyfin_tailscale_service_name }}"
jellyfin_tailscale_auth_key:

# ===============================
#       NGINX-PROXY-MANAGER
# ===============================
# Versions:
# * https://hub.docker.com/r/jc21/nginx-proxy-manager/tags
# -----
# # Optional path to extra configuration .env.j2 file
# nginx_proxy_manager_extra_envfile: "{{ playbook_dir }}/resources/npm/extra.env.j2"
nginx_proxy_manager_managed: false
nginx_proxy_manager_enabled: false
nginx_proxy_manager_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_manager_owner: "{{ ansible_user_id }}"
nginx_proxy_manager_conf_dir: ~/conf/nginx-proxy-manager
nginx_proxy_manager_service_name: nginx-proxy-manager
nginx_proxy_manager_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_vhost: "{{ nginx_proxy_manager_service_name }}.domain.local"
nginx_proxy_manager_extra_envfile:

# =======================
#       NGINX-PROXY
# =======================
# Versions:
# * https://hub.docker.com/r/jwilder/nginx-proxy/tags?name=alpine
# Exposes:
# * nginx_proxy_net - proxy network name
# -----
nginx_proxy_managed: false
nginx_proxy_enabled: false
nginx_proxy_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_owner: "{{ ansible_user_id }}"
nginx_proxy_service_name: nginx-proxy
nginx_proxy_hostname: "{{ nginx_proxy_service_name }}"
nginx_proxy_http_port:      # Ex.: '80', no HTTP when empty
nginx_proxy_https_port:     # Ex.: '443', no HTTPS when empty
nginx_proxy_certs_dir:      # Path to certs on the managed machine. Works paired with HTTPS

# ===========================
#       PORTAINER-AGENT
# ===========================
# Versions:
# * https://hub.docker.com/r/portainer/agent/tags?name=alpine
# -----
portainer_agent_managed: false
portainer_agent_enabled: false
portainer_agent_version:        # Leave blank to leverage bookshelf configuration
portainer_agent_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_agent_owner: "{{ ansible_user_id }}"
portainer_agent_conf_dir: ~/conf/portainer-agent
portainer_agent_service_name: portainer-agent
portainer_agent_hostname: "{{ portainer_service_name }}"
portainer_agent_host_management: false
portainer_agent_tcp_port:       # Leave blank to leverage bookshelf configuration

# =====================
#       PORTAINER
# =====================
# Versions:
# * https://hub.docker.com/r/portainer/portainer-ce/tags?name=alpine
# -----
portainer_managed: false
portainer_enabled: false
portainer_version:        # Leave blank to leverage bookshelf configuration
portainer_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_owner: "{{ ansible_user_id }}"
portainer_conf_dir: ~/conf/portainer
portainer_service_name: portainer
portainer_hostname: "{{ portainer_service_name }}"
portainer_vhost: "{{ portainer_service_name }}.domain.local"
portainer_web_ui_port:    # Leave blank to leverage bookshelf configuration
# Publish ssh tunnel server port
portainer_agent_ready: false

# =====================
#       TAILSCALE
# =====================
# Versions:
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
# Meant for tailscale server configuration: https://tailscale.com/kb/1352/servers
#
# IMPORTANT 1: Auth key is required for initial container run:
# https://tailscale.com/admin/settings/keys -> 'Generate auth key...'
#   -> generate single time or reusable
#
# IMPORTANT 2: After deployment https://tailscale.com/admin/machines -> 3 dots
# next to the machine -> Disable key expiry (to avoid re-login to tailscale)
#
# IMPORTANT 3: With changed routes or exit node params service requires a valid
# auth key, i.e. if it's one-timer new one must be provided
#
# NOTE 1: The service is useless unless it's configured for exit node or subnet
# router. Both need to be confirmed in the machines admin
tailscale_managed: false
tailscale_enabled: false
tailscale_version:          # Leave blank to leverage bookshelf configuration
tailscale_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
tailscale_owner: "{{ ansible_user_id }}"
tailscale_conf_dir: ~/conf/tailscale
tailscale_service_name: tailscale
tailscale_hostname: "{{ tailscale_service_name }}"
# Hostname within tailnet
tailscale_ts_hostname: "{{ tailscale_service_name }}"
tailscale_auth_key:
# Ex.: '192.168.0.0/24,192.168.2.0/24'
tailscale_routes:
tailscale_exit_node: false  # Advertise exit node
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/tailscale/extra.env.j2"
tailscale_extra_envfile:

# ====================
#       TWINGATE
# ====================
# Versions:
# * https://hub.docker.com/r/twingate/connector/tags
# -----
twingate_managed: false
twingate_enabled: false
twingate_version:         # Leave blank to leverage bookshelf configuration
twingate_compose_dir:     # Unless you know ..., leave blank to leverage bookshelf configuration
twingate_owner: "{{ ansible_user_id }}"
twingate_service_name: twingate
twingate_hostname: "{{ twingate_service_name }}"
twingate_tenant_name:     # Your twingate network name. Required
# https://<tenant>.twingate.com/networks/overview -> 'Deploy ... Connector'
#   -> 'Docker' -> 'Generate Tokens'
twingate_access_token:    # Required
twingate_refresh_token:   # Required
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/twingate/extra.env.j2"
twingate_extra_envfile:

# ===================
#       WG-EASY
# ===================
# Versions:
# * https://github.com/wg-easy/wg-easy/pkgs/container/wg-easy/versions?filters%5Bversion_type%5D=tagged
# Options description:
# * https://github.com/wg-easy/wg-easy/tree/production?tab=readme-ov-file#options
# * https://github.com/wg-easy/wg-easy/blob/production/How_to_generate_an_bcrypt_hash.md
# -----
wg_easy_managed: false
wg_easy_enabled: false
wg_easy_version:        # Leave blank to leverage bookshelf configuration
wg_easy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
wg_easy_owner: "{{ ansible_user_id }}"
wg_easy_conf_dir: ~/conf/wg-easy
wg_easy_service_name: wg-easy
wg_easy_hostname: "{{ wg_easy_service_name }}"
wg_easy_vhost: "{{ wg_easy_service_name }}.domain.local"
# Defaults to defined in WG-Easy. Ex.: '8.4.4.8,10.0.0.1'
wg_easy_dns:
# Defaults to no-pass login. See options description for how to generate
wg_easy_pass_hash:
# Required. Ex.: 'my-vpn-server.public-domain.com' or '112.2.84.234'
wg_easy_host:
wg_easy_web_ui_port:    # Leave blank to leverage bookshelf configuration
wg_easy_udp_port: 51820
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/wg-easy/extra.env.j2"
wg_easy_extra_envfile:



###########################
###       DESKTOP       ###
###########################

# ====================
#       AUDACITY
# ====================
audacity_managed: false

# =================
#       BRAVE
# =================
brave_managed: false

# =================
#       COPYQ
# =================
# For platforms different from ubuntu probably better flatpak.
# -----
# copyq_install_method: pm  # <- For package manager or 'flatpak'
# -----
# copyq_users:  # <- Existing users to have copyq autostarted
#   - "{{ ansible_user_id }}"
copyq_managed: false
copyq_install_method: pm
copyq_users:

# ===================
#       FLATPAK
# ===================
# flatpak_repos:  # <- These are all supported repos
#   - flathub
flatpak_managed: false
flatpak_repos:

# ================
#       GIMP
# ================
gimp_managed: false

# ====================
#       KDENLIVE
# ====================
kdenlive_managed: false
# 'snap' or 'flatpak'
kdenlive_install_method: snap

# ====================
#       NXPLAYER
# ====================
nxplayer_managed: false

# ======================
#       OBS-STUDIO
# ======================
obs_studio_managed: false

# =================
#       OPERA
# =================
opera_managed: false

# ====================
#       PHPSTORM
# ====================
phpstorm_managed: false

# =================
#       PINTA
# =================
pinta_managed: false
# 'snap' or 'flatpak'
pinta_install_method: flatpak

# ===================
#       SUBLIME
# ===================
sublime_managed: false
sublime_merge: false

# =================
#       TEAMS
# =================
# teams_users:  # <- Existing users to have teams autostarted
#   - "{{ ansible_user_id }}"
teams_managed: false
teams_users:

# =================
#       VIBER
# =================
# viber_users:  # <- Existing users to have viber autostarted
#   - "{{ ansible_user_id }}"
viber_managed: false
viber_users:

# ===============
#       VLC
# ===============
vlc_managed: false

# ==================
#       VSCODE
# ==================
vscode_managed: false
