---
# *_managed vars denote whether ansible takes care of the application or not.
#   `X_managed: false` application will still fire if it is in `Y_managed: true`
#   application dependencies
#
# *_enabled vars denote whether the application is enabled or not. Unlike
#   X_managed it disables the application when `X_enabled: false`

###
### Server connection configuration
###
# Don't keep passwords in plain text, encrypt them. One option would be:
#   ansible-vault encrypt_string -J PASSWORD_PLACEHOLDER
ansible_user: ansible-guy   # <- Not required for ssh-key-based connection
ansible_password: changeme  # <- Not required for ssh-key-based connection
ansible_become_password: changeme

# All before this ROLES_CONF_TS4LE64m91 marker stays
# unchanged by sample-vars.sh script
#################### {{ ROLES_CONF_TS4LE64m91 }} ####################



########################
###       BASE       ###
########################

# ===============
#       AGE
# ===============
# Simple, modern and secure file encryption tool.
#   https://github.com/FiloSottile/age
# -----
# VERSIONS: https://github.com/FiloSottile/age/releases
# -----
age_managed: false
age_version:    # Leave blank to leverage bookshelf configuration

# =============================
#       ALERT-MAX-TRAFFIC
# =============================
alert_max_traffic_managed: false
# notify-sh configuration
alert_max_traffic_provider:    # (REQUIRED) See supported in notify-sh role
alert_max_traffic_token:       # (REQUIRED)
alert_max_traffic_server:      # (REQUIRED for gotify)
alert_max_traffic_chat_id:     # (REQUIRED for telegram)
# Traffic alert settings
alert_max_traffic_interface:   # (REQUIRED) Network interface
# Max total traffic in MiB
alert_max_traffic_max_day:     # (OPTIONAL)
alert_max_traffic_max_month:   # (OPTIONAL)

# =====================
#       ALERT-SSH
# =====================
# SSH login notificator.
# Inspired by https://www.youtube.com/watch?v=VzH4NfJMiQc
# -----
alert_ssh_managed: false   #  "{{ factum_os_family not in ['alpine'] }}"   # <- Not supported by Alpine
alert_ssh_provider:        # (REQUIRED) See supported in notify-sh role
alert_ssh_token:           # (REQUIRED)
alert_ssh_server:          # (REQUIRED for gotify)
alert_ssh_chat_id:         # (REQUIRED for telegram)

# =====================
#       BASH-SANE
# =====================
# Sane bash defaults
# -----
bash_sane_managed: false
bash_sane_users:    # <- Existing users to use bash set settings
  # - "{{ ansible_user_id }}"

# ================
#       BASH
# ================
bash_managed: false
bash_completion: false
bash_users:       # <- Existing users to use bash by default
  # - "{{ ansible_user_id }}"

# ========================
#       DOCKER-CLEAN
# ========================
docker_clean_managed: false   # "{{ docker_managed | default(false) }}"   # <- When docker installed
docker_clean_logrotate_keep_count: 7
docker_clean_prune_images: false    # <- Weekly prune unused images

# ==================
#       DOCKER
# ==================
docker_managed: false
docker_users:     # <- Existing users to be added to docker group
  # - "{{ ansible_user_id }}"

# =========================
#       DYDNS-UPDATER
# =========================
# For more details see dydns.sh under files
# -----
dydns_updater_managed: false
dydns_updater_prereqs: true
dydns_updater_providers:    # <- Supported list
  # - duckdns
  # - dynu
  # - now-dns
  # - ydns
dydns_updater_secret: |     # <- Will go to secret file
#   DUCKDNS_TOKEN=123   # (REQUIRED for given provider)
#   DYNU_TOKEN=456
#   # ... Same for other providers ...
dydns_updater_config: |     # <- Will go to config file
#   DUCKDNS_DOMAINS=(                    `# (REQUIRED)`
#     site1.duckdns.org
#     site2.duckdns.org
#   )
#   DUCKDNS_SCHEDULE='*/10 * * * *'       # (OPTIONAL)
#   DUCKDNS_ERR_TITLE='DuckDNS failure'   # (OPTIONAL)
#   # ... Same for other providers ...
# Extra providers to go to extra file
dydns_updater_extra:        # "{{ lookup('file', playbook_dir + '/resources/dydns.extra.sh') }}"
# Update failure alert settings. Fields marked as REQUIRED
# are only required when provider is set
dydns_updater_alert_provider:       # See supported in notify-sh role
dydns_updater_alert_token:          # (REQUIRED)
dydns_updater_alert_server:         # (REQUIRED for gotify)
dydns_updater_alert_chat_id:        # (REQUIRED for telegram)

# =================
#       ENVAR
# =================
# Installs envar for bash
# -----
# Exposes:
#   * envar_etc_dir - Used by other tools
#   * envar_user_dir - Used by other tools
# -----
envar_managed: false
envar_users:    # <- Existing users to have access to envar
#   - "{{ ansible_user_id }}"
#   - root

# ===============
#       FZF
# ===============
# General-purpose command-line fuzzy finder.
#   https://github.com/junegunn/fzf
# -----
# VERSIONS: https://github.com/junegunn/fzf/releases
# -----
fzf_managed: false
fzf_version:    # Leave blank to leverage bookshelf configuration
fzf_users:      # <- Existing users to have fzf integrated
  # - "{{ ansible_user_id }}"
  # - root

# ===============
#       GIT
# ===============
git_managed: false
git_extraconf:
  #   # Required
  # - owner: "{{ ansible_user_id }}"    # <- Existing user. Must be unique
  #   source: gitconfig.extra.ini.j2    # <- In-role, handled as template due to '*.j2'
  #   # Optional variables, can be addressed in the template via 'conf' map in case of template source
  #   user_name: Foo                    # <- 'conf.user_name' in the template
  #   user_email: foo@bar.baz
  # - owner: another-user
  #   source: "{{ playbook_dir }}/resources/git/extra.ini"  # <- Handled as file due to not '*.j2'
  #   # No variables, as it's not a template
  # # ...

# =====================
#       NOTIFY-SH
# =====================
# Exposes:
#   * notify_sh_path - notify.sh script location
# -----
notify_sh_managed: false
# false is useful when notify.sh is for exemple intended to be mounted and used
# in a docker container. In such case docker host machine doesn't need prereqs
notify_sh_prereqs: false  # Install notify.sh prepreqs

# ===================
#       PS1-GIT
# ===================
# Installs fancy PS1 with git PS1 suffix support. Loaded with envar.
# Doesn't require git
# -----
ps1_git_managed: false
ps1_git_users:    # <- Existing users to have this PS1
  # - "{{ ansible_user_id }}"
  # - root

# =================
#       SNAPD
# =================
# NOTE:
#   Snap packages installation is not really supported in
#   OpenVZ containers (https://askubuntu.com/a/1340844). Although
#   snapd gets installed fine.
# ----------
snapd_managed: false  # "{{ factum_os_like in ['ubuntu'] }}"  # <- Only supported by Ubuntu like

# ============================
#       TAILSCALE-CLIENT
# ============================
# Tailscale makes creating software-defined networks easy: securely connecting
# users, services, and devices.
#   https://tailscale.com/
# -----
tailscale_client_managed: false

# ================
#       TMUX
# ================
tmux_managed: false
tmux_sane_users:  # <- Existing users to source my preferred configuration
  # - "{{ ansible_user_id }}"

# =================
#       TMUXP
# =================
# IMPORTANT:
#   The role doesn't support RHEL 10 family
# -----
tmuxp_managed: false  #  "{{ factum_os_family not in ['alpine'] }}"   # <- Not supported by Alpine
# See all available in templates/
tmuxp_conf:
  # - owner: "{{ ansible_user_id }}"  # <- Existing user. Must be unique
  #   conf:
  #       # Required
  #     - source: sensors.yaml.j2     # <- In-role, handled as template due to '*.j2'
  #       name: my-sensors            # <- Will land in ~/.tmuxp/my-sensors.yaml
  #       # Optional variables, can be addressed in the template via 'conf' map in case of template source
  #       session_name: Foo           # <- 'conf.session_name' in the template
  #       window_name: bar
  #     - source: "{{ playbook_dir }}/resources/tmuxp/demo1.yaml"   # <- Handled as file due to not '*.j2'
  #       name: demo1
  #       # No variables, as it's not a template
  # - owner: # ...
  #   # ...

# ====================
#       TOOLBELT
# ====================
# Contains very basic and often needed tools that for most often usage don't
# require any additional configuration. Some of the tools are categorized and
# the categories can be used to install the whole set of underlying tools.
# -----
# * downloaders:  curl, wget
# * editors:      nano, neovim, vim
# * viewers:      bat, glow
# * htop, jq, sensors, skate, speedtest, tar, wishlist
# -----
# * bat       : cat on steroids (https://github.com/sharkdp/bat)
# * glow      : Markdown reader (https://github.com/charmbracelet/glow), for
#               Alpine it's force excluded, as not supported yet.
#               TODO: add it to Alpine when supported
# * skate     : A personal key-value store (https://github.com/charmbracelet/skate)
# * wishlist  : The SSH directory (https://github.com/charmbracelet/wishlist).
#               Disabled for Alpine, as it's not available in its repo
# -----
# IMPORTANT:
#   The role speedtest tool is not supported in RHEL 10 family. I.e.:
#   toolbelt_exclude: [speedtest]  # <- For RHEL 10 family
# -----
toolbelt_managed: false
toolbelt_pick:      # <- Install only a fraction. Leave empty to install all
  # - nano            # <- nano will be installed
  # - downloaders     # <- All downloaders will be installed
  # - tar             # <- tar will be installed
toolbelt_exclude:   # <- Opposed to toolbelt_pick, higher priority than toolbelt_pick
  # - wget            # <- In conjunction with toolbelt_pick above, only curl will be installed
  # - editors         # <- Dispite toolbelt_pick above, nano will not be installed

# ========================
#       VIRT-MANAGER
# ========================
# Virtual Machine Manager
# -----
virt_manager_managed: false   # "{{ factum_os_family in ['debian'] }}"  # <- Only Debian family is supported
virt_manager_users:   # <- Existing users to be added to libvirt group
  # - "{{ ansible_user_id }}"

# ==================
#       VNSTAT
# ==================
# Console-based network traffic monitor that keeps a log of daily network
# traffic for the selected interface(s)
# -----
# IMPORTANT:
#   The role doesn't support RHEL 10 family
# -----
vnstat_managed: false



###########################
###       SERVICE       ###
###########################

# ===================
#       ADGUARD
# ===================
# The worldâ€™s most advanced ad blocker!
#   https://adguard.com/en/adguard-home/overview.html
# Configured only to work as DNS server
# -----
# IMPORTANT:
#   Fix for resolved:
#   https://hub.docker.com/r/adguard/adguardhome#resolved-daemon
# -----
# VERSIONS:
#   * https://hub.docker.com/r/adguard/adguardhome/tags
#   * => See tailscale role versions
# -----
adguard_managed: false
adguard_enabled: false
adguard_version:        # Leave blank to leverage bookshelf configuration
adguard_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
adguard_owner: "{{ ansible_user_id }}"
adguard_conf_dir: ~/conf/adguard
adguard_service_name: adguard
adguard_hostname: "{{ adguard_service_name }}"
adguard_vhost: "{{ adguard_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
adguard_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
adguard_init_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
adguard_dns_ports_exposed: true
adguard_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both adguard and tailscale.
adguard_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/adguard.env.j2') }}"
  # References:
  #   * Adguard is not really configurable via env vars
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
# ----------
# The following settings configure Adguard to run as a part of tailnet. This
# allows to configure adguard for a tailnet DNS server under
# https://tailscale.com/admin/dns -> Nameservers
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
#
# NOTE 1: Local network is not accessible within tailnet, which means that if i
# create an adguard DNS rewrite for 'my-site.local' to '192.168.0.20', the site
# will not be reachable in the tailnet. SOLUTION: deploy another tailscale
# service with "advertised routes" (https://tailscale.com/kb/1019/subnets)
adguard_tailscaled: false
adguard_ts_version:    # Leave blank to leverage bookshelf configuration
adguard_ts_service_name: "{{ adguard_service_name }}-ts"
adguard_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
adguard_ts_hostname: "{{ adguard_service_name }}"
adguard_ts_auth_key:

# ==========================
#       AUDIOBOOKSHELF
# ==========================
# Self-hosted audiobook and podcast server
#   https://www.audiobookshelf.org/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/advplyr/audiobookshelf/tags
#   * => See tailscale role versions
# -----
# NOTE 1:
#   * for audiobookshelf_data_volumes '~' is navidrome_user HOME
#   * for other directories '~' is audiobookshelf_owner HOME
# -----
# iOS clients:
#   * plappa - payed audiobooks download
#   * ShelfPlayer - payed app
#   * SoundLeaf
#   * https://www.audiobookshelf.org/faq/app/ - 3d party list
# -----
audiobookshelf_managed: false
audiobookshelf_enabled: false
audiobookshelf_version:       # Leave blank to leverage bookshelf configuration
audiobookshelf_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
audiobookshelf_owner: "{{ ansible_user_id }}"       # <- Who runs the container
audiobookshelf_user: "{{ audiobookshelf_owner }}"   # <- The in-container user and data filesystem owner
audiobookshelf_conf_dir: ~/conf/audiobookshelf
audiobookshelf_data_volumes:  # <- Volumes for data
  - ~/data/Audiobooks:/data   # (REQUIRED) At least one volume required
  # - ~/data/Children/Audiobooks:/extra/Children
  # - ~/data/Parents/Audiobooks:/extra/Parents
audiobookshelf_service_name: audiobookshelf
audiobookshelf_hostname: "{{ audiobookshelf_service_name }}"
audiobookshelf_vhost: "{{ audiobookshelf_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
audiobookshelf_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
audiobookshelf_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both audiobookshelf and tailscale.
audiobookshelf_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/audiobookshelf.env.j2') }}"
  # References:
  #   * https://www.audiobookshelf.org/docs#env-configuration
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # PORT=5555
# ----------
# The following settings configure Audiobookshelf to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
audiobookshelf_tailscaled: false
audiobookshelf_ts_version:   # Leave blank to leverage bookshelf configuration
audiobookshelf_ts_service_name: "{{ audiobookshelf_service_name }}-ts"
audiobookshelf_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
audiobookshelf_ts_hostname: "{{ audiobookshelf_service_name }}"
audiobookshelf_ts_auth_key:

# ==================
#       BLINKO
# ==================
# An open-source, self-hosted personal AI note tool prioritizing privacy,
# built using TypeScript.
#   https://blinko.space/en
# -----
# VERSIONS:
#   * https://hub.docker.com/r/blinkospace/blinko/tags
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
blinko_managed: false
blinko_enabled: false
blinko_version:             # Leave blank to leverage bookshelf configuration
blinko_postgres_version:    # Leave blank to leverage bookshelf configuration
blinko_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
blinko_owner: "{{ ansible_user_id }}"
blinko_conf_dir: ~/conf/blinko
blinko_service_name: blinko
blinko_hostname: "{{ blinko_service_name }}"
blinko_vhost: "{{ blinko_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
blinko_web_ui_port:         # Leave blank for bookshelf default, or '-1' to not expose the port
blinko_db_service_name: "{{ blinko_service_name }}-db"
blinko_db_pass:             # (REQUIRED) Something hard and random
blinko_nextauth_secret:     # (REQUIRED) Something hard and random
blinko_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Blinko and Tailscale.
blinko_extra_env: |         # "{{ lookup('template', playbook_dir + '/resources/blinko.env.j2') }}"
  # References:
  #   * https://docs.blinko.space/en/install#docker-installation
  #   * https://hub.docker.com/_/postgres#environment-variables
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # NEXTAUTH_URL=...
# ----------
# The following settings configure Blinko to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
blinko_tailscaled: false
blinko_ts_version:          # Leave blank to leverage bookshelf configuration
blinko_ts_service_name: "{{ blinko_service_name }}-ts"
blinko_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
blinko_ts_hostname: "{{ blinko_service_name }}"
blinko_ts_auth_key:

# =================
#       CADDY
# =================
# Fast and extensible multi-platform HTTP/1-2-3 web server with automatic HTTPS
#   https://caddyserver.com/
# For basic scenarios see README.md in the role
# -----
# VERSIONS:
#   * https://hub.docker.com/_/caddy/tags
# -----
caddy_managed: false
caddy_enabled: false
caddy_version:          # Leave blank to leverage bookshelf configuration
caddy_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
caddy_owner: "{{ ansible_user_id }}"
caddy_conf_dir: ~/conf/caddy
caddy_service_name: caddy
caddy_hostname: "{{ caddy_service_name }}"
caddy_modules:          # (OPTIONAL) https://caddyserver.com/docs/modules/
  # - github.com/caddy-dns/desec
  # - github.com/mholt/caddy-l4
  # - github.com/caddy-dns/duckdns
caddy_cert_bundles:
  # - text: "{{ lookup('file', 'demo1.bundle.pem.b64') | b64decode }}"
  #   name: domain.local.pem    # <- Will land in /etc/ssl/private/domain.local.pem
# Can be string or list. Ex.:
#   caddy_caddyfile: "{{ lookup('file', 'demo1.Caddyfile') }}"
#   caddy_caddyfile: ["{{ lookup('file', 'demo1.Caddyfile') }}"]
caddy_caddyfile:
  - "{{ lookup('file', 'demo1.Caddyfile') }}"   # (REQUIRED) At least one
  # - "{{ lookup('file', 'demo2.Caddyfile') }}"
caddy_host_net: false
# Port settings only get applied with caddy_host_net == false
# NOTE: Avoid port conflicts with other reverse proxies
caddy_ports:            # (OPTIONAL) HOST:CONTAINER ports mapping
  - 80:80               # <- HTTP
  - 443:443             # <- HTTPS
  - 3128:3128           # <- SPICE port
caddy_vhost: "{{ caddy_service_name }}.domain.local"
# Optional extra env vars
caddy_extra_env: |      # "{{ lookup('template', playbook_dir + '/resources/caddy.env.j2') }}"
  # Can be used to pass values to Caddyfile
  # ----------
  # DESEC_BASE=demo.dedyn.io
caddy_secrets_env: |      # "{{ lookup('template', playbook_dir + '/resources/caddy.secret.env.j2') }}"
  # Can be used to pass secret values to Caddyfile
  # ----------
  # DESEC_TOKEN=123qwerty456
# ----------
# The following settings configure Caddy to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
caddy_tailscaled: false
caddy_ts_version:          # Leave blank to leverage bookshelf configuration
caddy_ts_service_name: "{{ caddy_service_name }}-ts"
caddy_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
caddy_ts_hostname: "{{ caddy_service_name }}"
caddy_ts_auth_key:

# =======================
#       CODE-SERVER
# =======================
# VS Code in the browser
#   https://coder.com/docs/code-server
#
# Unlike most of services gets installed in the system (not docker). The reason
# is that it's simpler to manage runtime (add extensions and install their
# dependency packages) on a bare metal setup.
# -----
# VERSIONS: https://github.com/coder/code-server/releases
# -----
code_server_managed: false  # "{{ factum_os_family not in ['alpine'] }}"  # <- Not supported by Alpine
code_server_version:      # Leave blank to leverage bookshelf configuration
code_server_owner: "{{ ansible_user_id }}"
code_server_web_ui_port:  # Leave blank to leverage bookshelf configuration
code_server_extensions:   # <- List of extension
  # - editorconfig.editorconfig
  # - mads-hartmann.bash-ide-vscode
  # - timonwong.shellcheck
  # - foxundermoon.shell-format
  # - redhat.ansible
code_server_pass: changeme
# If defined, takes precedence over 'code_server_pass'.
# Generate with (replace PASS with your value):
#   printf -- '%s' 'PASS' | docker container run -i --rm alpine /bin/sh -c 'apk add --update --no-cache argon2 openssl && argon2 "$(openssl rand -base64 8)" -e'
code_server_hashed_pass:
# Custom config. See the default one in templates/config.yaml.j2.
# templates/config.base.yaml.j2 is always appended in the end.
code_server_custom_conf:    # "{{ lookup('template', playbook_dir + '/resources/code-server.config.yaml.j2') }}"
# Custom user config, applied only on the service initialization.
# See the default one in templates/sane.json.j2.
code_server_user_conf:      # "{{ lookup('template', 'sane.json.j2') }}"

# ======================
#       CRONMASTER
# ======================
# Cron management made easy
#   https://github.com/fccview/cronmaster
# -----
# VERSIONS:
#   * https://github.com/fccview/cronmaster/pkgs/container/cronmaster
#   * => See tailscale role versions
# -----
cronmaster_managed: false
cronmaster_enabled: false
cronmaster_version:             # Leave blank to leverage bookshelf configuration
cronmaster_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
cronmaster_owner: "{{ ansible_user_id }}"
cronmaster_conf_dir: ~/conf/cronmaster
cronmaster_service_name: cronmaster
cronmaster_hostname: "{{ cronmaster_service_name }}"
cronmaster_vhost: "{{ cronmaster_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
cronmaster_web_ui_port:         # Leave blank for bookshelf default, or '-1' to not expose the port
cronmaster_auth_pass:           # (REQUIRED)
cronmaster_crontab_users:       # List of crontab users (from host)
  # - root
  # - "{{ ansible_user_id }}"
cronmaster_scripts:
  # - text: "{{ lookup('template', 'demo-script.sh.j2') }}"  # <- See this one for demo, uses demo-secret.txt
  #   filename: demo-script.sh        # Destination filename, should not intersect with existing scripts
cronmaster_snippets:  # <- Guide: https://github.com/fccview/cronmaster/blob/main/snippets/README.md
  # - text: "{{ lookup('file', 'demo-snippet.sh') }}"
  #   filename: demo-snippet.sh
cronmaster_secrets:
  # - text: "{{ lookup('file', 'demo-secret.txt') }}"
  #   filename: demo-secret.txt       # See demo-script.sh.j2 to see how to reference the secret
  #   # owner: "{{ ansible_user_id }}"  # Optional, defaults to root. The file is only RW by the owner
# Optional extra env vars, will affect both CronMaster and Tailscale.
cronmaster_extra_env: |         # "{{ lookup('template', playbook_dir + '/resources/cronmaster.env.j2') }}"
  # References:
  #   * https://github.com/fccview/cronmaster#environment-variables
  #   * https://hub.docker.com/_/postgres#environment-variables
  # ----------
  # NEXT_PUBLIC_CLOCK_UPDATE_INTERVAL=60000
# ----------
# The following settings configure CronMaster to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
cronmaster_tailscaled: false
cronmaster_ts_version:          # Leave blank to leverage bookshelf configuration
cronmaster_ts_service_name: "{{ cronmaster_service_name }}-ts"
cronmaster_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
cronmaster_ts_hostname: "{{ cronmaster_service_name }}"
cronmaster_ts_auth_key:

# =====================
#       DUPLICATI
# =====================
# Store securely encrypted backups in the cloud!
#   https://duplicati.com/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/duplicati/tags?name=-ls
#   * => See tailscale role versions
# -----
duplicati_managed: false
duplicati_enabled: false
duplicati_version:        # Leave blank to leverage bookshelf configuration
duplicati_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
duplicati_owner: "{{ ansible_user_id }}"    # <- Who runs the container
duplicati_conf_dir: ~/conf/duplicati
duplicati_backup_volumes:                   # <- Mounts for backups, at least one entry required
  - ~/backup/Main:/backups                  # <- One of data volumes must be mounted to '/backups'
  # - ~/backup/Photos:/extra/backups/Photos
  # - ~/backup/Important:/extra/backups/Important
# '~' refers to the owner HOME.
duplicati_data_volumes:                     # <- Mounts for data, at least one entry required
  - volume: ~/data:/source                  # <- One of data volumes must be mounted to '/source'
    owner: "{{ duplicati_owner }}"          # <- (OPTIONAL) Host directory owner
  # - volume: ~/data1/Photos:/extra/source/Photos    # <- Owner is duplicati_owner
  # - ~/data1/Important:/extra/source/Important      # <- Owner is duplicati_owner
duplicati_service_name: duplicati
duplicati_hostname: "{{ duplicati_service_name }}"
duplicati_vhost: "{{ duplicati_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
duplicati_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
duplicati_enc_key:        # (REQUIRED) Min 8 symbols
duplicati_pass:           # (OPTIONAL)
duplicati_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Duplicati and tailscale.
duplicati_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/duplicati.env.j2') }}"
  # References:
  #   * https://prev-docs.duplicati.com/en/latest/07-other-command-line-utilities/
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # CLI_ARGS=--server-datafolder=/root/.config/Duplicati
# ----------
# The following settings configure Duplicati to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
duplicati_tailscaled: false
duplicati_ts_version:           # Leave blank to leverage bookshelf configuration
duplicati_ts_service_name: "{{ duplicati_service_name }}-ts"
duplicati_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
duplicati_ts_hostname: "{{ duplicati_service_name }}"
duplicati_ts_auth_key:

# =======================
#       FILEBROWSER
# =======================
# Web File Browser
#   https://filebrowser.org/
# ALTERNATIVE (when time comes);
#   https://github.com/gtsteffaniak/filebrowser
# -----
# VERSIONS:
#   * https://hub.docker.com/r/filebrowser/filebrowser/tags?name=-s6
#   * => See tailscale role versions
# -----
# NOTE 1:
#   * for audiobookshelf_data_dir '~' is audiobookshelf_user HOME
#   * for other directories '~' is audiobookshelf_owner HOME
# NOTE 2: Temporary password for the admin user will be printed to the container
# log on startup
# -----
filebrowser_managed: false
filebrowser_enabled: false
filebrowser_version:        # Leave blank to leverage bookshelf configuration
filebrowser_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
filebrowser_owner: "{{ ansible_user_id }}"    # <- Who runs the container
filebrowser_user: "{{ filebrowser_owner }}"   # <- The in-container user and data filesystem owner
filebrowser_conf_dir: ~/conf/filebrowser
filebrowser_data_dir: ~/data
filebrowser_service_name: filebrowser
filebrowser_hostname: "{{ filebrowser_service_name }}"
filebrowser_vhost: "{{ filebrowser_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
filebrowser_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
filebrowser_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both filebrowser and tailscale.
filebrowser_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/filebrowser.env.j2') }}"
  # References:
  #   * FileBrowser is not really configurable via env vars
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
# ----------
# The following settings configure FileBrowser to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
filebrowser_tailscaled: false
filebrowser_ts_version:           # Leave blank to leverage bookshelf configuration
filebrowser_ts_service_name: "{{ filebrowser_service_name }}-ts"
filebrowser_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
filebrowser_ts_hostname: "{{ filebrowser_service_name }}"
filebrowser_ts_auth_key:

# =================
#       GATUS
# =================
# Automated developer-oriented status page.
#   https://github.com/TwiN/gatus
# -----
# VERSIONS:
#   * https://github.com/twin/gatus/pkgs/container/gatus
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
gatus_managed: false
gatus_enabled: false
gatus_version:            # Leave blank to leverage bookshelf configuration
gatus_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
gatus_owner: "{{ ansible_user_id }}"
gatus_conf_dir: ~/conf/gatus
gatus_service_name: gatus
gatus_hostname: "{{ gatus_service_name }}"
gatus_vhost: "{{ gatus_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gatus_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
gatus_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Can be string or list. Ex.:
#   gatus_config: "{{ lookup('file', 'demo.yaml') }}"
#   gatus_config: ["{{ lookup('file', 'demo.yaml') }}"]
gatus_config:
  - "{{ lookup('file', 'demo.yaml') }}"   # <- (REQUIRED) At least one
  # - |
  #   endpoints:
  #     - name: Dahoo (demo)
  #       group: Demos
  #       url: "https://yahoo.com"
  #       conditions: ["[STATUS] >= 200", "[STATUS] < 400"]
# Optional extra env vars, will affect both Gatus and Tailscale.
gatus_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/gatus.env.j2') }}"
  # Gatus allows $ENV_VAR in the config files
  # ----------
  # SOME_VAR=some-val
gatus_secrets_env: |      # "{{ lookup('template', playbook_dir + '/resources/gatus.secret.env.j2') }}"
  # Same as extra env, but for secrets
  # ----------
  # SOME_SECRET_VAR=some-secret
# ----------
# The following settings configure Gatus to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gatus_tailscaled: false
gatus_ts_version:           # Leave blank to leverage bookshelf configuration
gatus_ts_service_name: "{{ gatus_service_name }}-ts"
gatus_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gatus_ts_hostname: "{{ gatus_service_name }}"
gatus_ts_auth_key:

# ==================
#       GOKAPI
# ==================
# Lightweight selfhosted Firefox Send alternative without public upload. AWS S3 supported.
#   https://github.com/Forceu/gokapi
# -----
# VERSIONS:
#   * https://hub.docker.com/r/f0rc3/gokapi/tags
#   * => See tailscale role versions
# -----
gokapi_managed: false
gokapi_enabled: false
gokapi_version:            # Leave blank to leverage bookshelf configuration
gokapi_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
gokapi_owner: "{{ ansible_user_id }}"
gokapi_conf_dir: ~/conf/gokapi
gokapi_service_name: gokapi
gokapi_hostname: "{{ gokapi_service_name }}"
gokapi_vhost: "{{ gokapi_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gokapi_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
gokapi_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Gokapi and Tailscale.
gokapi_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/gokapi.env.j2') }}"
  # References:
  #   * https://gokapi.readthedocs.io/en/latest/advanced.html#environment-variables
  # NOTES:
  #   * Don't use the following variables:
  #     * DOCKER_NONROOT
  #     * GOKAPI_PORT
  # ----------
  # GOKAPI_CHUNK_SIZE_MB=80
# ----------
# The following settings configure Gokapi to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gokapi_tailscaled: false
gokapi_ts_version:          # Leave blank to leverage bookshelf configuration
gokapi_ts_service_name: "{{ gokapi_service_name }}-ts"
gokapi_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gokapi_ts_hostname: "{{ gokapi_service_name }}"
gokapi_ts_auth_key:

# ==================
#       GOTIFY
# ==================
# A simple server for sending and receiving messages
#   https://gotify.net/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/gotify/server/tags
#   * => See tailscale role versions
# -----
gotify_managed: false
gotify_enabled: false
gotify_version:       # Leave blank to leverage bookshelf configuration
gotify_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
gotify_owner: "{{ ansible_user_id }}"
gotify_conf_dir: ~/conf/gotify
gotify_service_name: gotify
gotify_hostname: "{{ gotify_service_name }}"
gotify_vhost: "{{ gotify_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gotify_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
gotify_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Gotify and tailscale.
gotify_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/gotify.env.j2') }}"
  # References:
  #   * https://gotify.net/docs/config#environment-variables
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # GOTIFY_REGISTRATION=false
# ----------
# The following settings configure Gotify to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gotify_tailscaled: false
gotify_ts_version:           # Leave blank to leverage bookshelf configuration
gotify_ts_service_name: "{{ gotify_service_name }}-ts"
gotify_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gotify_ts_hostname: "{{ gotify_service_name }}"
gotify_ts_auth_key:

# ====================
#       HEDGEDOC
# ====================
# Ideas grow better together
#   https://hedgedoc.org/
# -----
# VERSIONS: https://quay.io/repository/hedgedoc/hedgedoc?tab=tags
# -----
# NOTES:
#   * container is quite restricted. See restrictions in the main.env.j2 tempate.
#     They can be overriden with extra_env
#   * with restrictions in place users registration with ./bin/manage_users tool.
# -----
hedgedoc_managed: false
hedgedoc_enabled: false
hedgedoc_version:       # Leave blank to leverage bookshelf configuration
hedgedoc_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
hedgedoc_owner: "{{ ansible_user_id }}"
hedgedoc_conf_dir: ~/conf/hedgedoc
hedgedoc_service_name: hedgedoc
hedgedoc_hostname: "{{ hedgedoc_service_name }}"
hedgedoc_vhost: "{{ hedgedoc_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
hedgedoc_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
hedgedoc_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# HedgeDoc domain. Ex.: {{ hedgedoc_vhost }} or {{ ansible_default_ipv4.address }}
hedgedoc_domain:        # (REQUIRED)
# Optional extra env vars
hedgedoc_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/hedgedoc.env.j2') }}"
  # References:
  #   * https://docs.hedgedoc.org/guides/reverse-proxy/#hedgedoc-config
  # ----------
  # CMD_URL_ADDPORT=false
  # CMD_PROTOCOL_USESSL=true
  # CMD_ALLOW_EMAIL_REGISTER=true   # <- Allow users registration

# ====================
#       HEIMDALL
# ====================
# Application Dashboard
#   https://heimdall.site/
# -----
# VERSIONS: https://hub.docker.com/r/linuxserver/heimdall/tags
# -----
heimdall_managed: false
heimdall_enabled: false
heimdall_version:       # Leave blank to leverage bookshelf configuration
heimdall_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
heimdall_owner: "{{ ansible_user_id }}"
heimdall_conf_dir: ~/conf/heimdall
heimdall_service_name: heimdall
heimdall_hostname: "{{ heimdall_service_name }}"
heimdall_vhost: "{{ heimdall_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
heimdall_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
heimdall_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Ex.:
#   # heimdall_web_ui_port_ext - contains actual external port
#   http://{{ ansible_default_ipv4.address }}:{{ heimdall_web_ui_port_ext }}
#   https://{{ heimdall_vhost }}
heimdall_base_url:      # (REQUIRED)

# ====================
#       JELLYFIN
# ====================
# The Free Software Media System
#   https://jellyfin.org/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/jellyfin/tags
#   * => See tailscale role versions
# -----
# NOTE 1:
#   * for jellyfin_data_volumes '~' is navidrome_user HOME
#   * for other directories '~' is jellyfin_owner HOME
# -----
jellyfin_managed: false
jellyfin_enabled: false
jellyfin_version:       # Leave blank to leverage bookshelf configuration
jellyfin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
jellyfin_owner: "{{ ansible_user_id }}"   # <- Who runs the container
jellyfin_user: "{{ jellyfin_owner }}"     # <- The in-container user and data filesystem owner
jellyfin_conf_dir: ~/conf/jellyfin
jellyfin_data_volumes:    # <- Volumes for data
  - ~/data/Movies:/data   # (REQUIRED) At least one volume required
  # - ~/data/Parents/Movies:/extra/Parents
  # - ~/data/Children/Movies:/extra/Children
jellyfin_service_name: jellyfin
jellyfin_hostname: "{{ jellyfin_service_name }}"
jellyfin_vhost: "{{ jellyfin_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
jellyfin_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
jellyfin_discovery_ports_exposed: true
jellyfin_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both jellyfin and tailscale.
jellyfin_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/jellyfin.env.j2') }}"
  # References:
  #   * https://hub.docker.com/r/linuxserver/jellyfin#parameters
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # JELLYFIN_PublishedServerUrl=https://{{ jellyfin_vhost }}
# ----------
# The following settings configure Jellyfin to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jellyfin_tailscaled: false
jellyfin_ts_version:            # Leave blank to leverage bookshelf configuration
jellyfin_ts_service_name: "{{ jellyfin_service_name }}-ts"
jellyfin_ts_funnelled: false    # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
jellyfin_ts_hostname: "{{ jellyfin_service_name }}"
jellyfin_ts_auth_key:

# =========================
#       JOPLIN-SERVER
# =========================
# Open source note-taking app. Capture your thoughts and securely access them from any device.
#   https://joplinapp.org/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/joplin/server/tags
#   * https://hub.docker.com/_/postgres/tags?name=alpine
# -----
joplin_server_managed: false
joplin_server_enabled: false
joplin_server_version:              # Leave blank to leverage bookshelf configuration
joplin_server_postgres_version:     # Leave blank to leverage bookshelf configuration
joplin_server_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
joplin_server_owner: "{{ ansible_user_id }}"
joplin_server_conf_dir: ~/conf/joplin-server
joplin_server_service_name: joplin-server
joplin_server_hostname: "{{ joplin_server_service_name }}"
joplin_server_vhost: "{{ joplin_server_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
joplin_server_web_ui_port:          # Leave blank for bookshelf default, or '-1' to not expose the port
# Joplin base url. Ex.:
#   # joplin_server_web_ui_port_ext - contains actual external port
#   https://{{ joplin_server_vhost }}
#   http://{{ ansible_default_ipv4.address }}:{{ joplin_server_web_ui_port_ext }}
joplin_server_base_url:             # (REQUIRED)
joplin_server_db_service_name: "{{ joplin_server_service_name }}-db"
joplin_server_db_pass:              # (REQUIRED)
joplin_server_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# =================
#       JOTTY
# =================
# A simple, self-hosted app for your checklists and notes.
#   https://jotty.page/
# -----
# VERSIONS:
#   * https://github.com/fccview/jotty/pkgs/container/jotty
#   * => See tailscale role versions
# -----
jotty_managed: false
jotty_enabled: false
jotty_version:            # Leave blank to leverage bookshelf configuration
jotty_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
jotty_owner: "{{ ansible_user_id }}"
jotty_conf_dir: ~/conf/jotty
jotty_service_name: jotty
jotty_hostname: "{{ jotty_service_name }}"
jotty_vhost: "{{ jotty_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
jotty_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Jotty and Tailscale.
jotty_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/jotty.env.j2') }}"
  # References:
  #   * https://github.com/fccview/jotty/blob/main/howto/DOCKER.md#environment-variables
  # ----------
  # SERVE_PUBLIC_IMAGES=yes
# ----------
# The following settings configure Jotty to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jotty_tailscaled: false
jotty_ts_version:          # Leave blank to leverage bookshelf configuration
jotty_ts_service_name: "{{ jotty_service_name }}-ts"
jotty_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
jotty_ts_hostname: "{{ jotty_service_name }}"
jotty_ts_auth_key:

# ==================
#       MEALIE
# ==================
# Mealie is a self hosted recipe manager and meal planner with a RestAPI backend
# and a reactive frontend application built in Vue for a pleasant user
# experience for the whole family. Easily add recipes into your database by
# providing the url and mealie will automatically import the relevant data or
# add a family recipe with the UI editor
#   https://mealie.io/
# -----
# VERSIONS:
#   * https://github.com/mealie-recipes/mealie/pkgs/container/mealie
#   * => See tailscale role versions
# -----
mealie_managed: false
mealie_enabled: false
mealie_version:       # Leave blank to leverage bookshelf configuration
mealie_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
mealie_owner: "{{ ansible_user_id }}"
mealie_conf_dir: ~/conf/mealie
mealie_service_name: mealie
mealie_hostname: "{{ mealie_service_name }}"
mealie_vhost: "{{ mealie_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
mealie_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
mealie_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Mealie and tailscale.
mealie_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/mealie.env.j2') }}"
  # References:
  #   * https://docs.mealie.io/documentation/getting-started/installation/backend-config/
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # ALLOW_SIGNUP=false
# ----------
# The following settings configure Mealie to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
mealie_tailscaled: false
mealie_ts_version:          # Leave blank to leverage bookshelf configuration
mealie_ts_service_name: "{{ mealie_service_name }}-ts"
mealie_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
mealie_ts_hostname: "{{ mealie_service_name }}"
mealie_ts_auth_key:

# =================
#       MEMOS
# =================
# A modern, open-source, self-hosted knowledge management and note-taking
# platform designed for privacy-conscious users and organizations.
#   https://www.usememos.com/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/neosmemo/memos/tags
#   * => See tailscale role versions
# -----
memos_managed: false
memos_enabled: false
memos_version:        # Leave blank to leverage bookshelf configuration
memos_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
memos_owner: "{{ ansible_user_id }}"
memos_conf_dir: ~/conf/memos
memos_service_name: memos
memos_hostname: "{{ memos_service_name }}"
memos_vhost: "{{ memos_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
memos_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Memos and tailscale.
memos_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/memos.env.j2') }}"
  # References:
  #   * https://www.usememos.com/docs/configuration
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # MEMOS_MAX_LOGIN_ATTEMPTS=5
  # MEMOS_LOCKOUT_DURATION=900
# ----------
# The following settings configure Memos to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
memos_tailscaled: false
memos_ts_version:           # Leave blank to leverage bookshelf configuration
memos_ts_service_name: "{{ memos_service_name }}-ts"
memos_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
memos_ts_hostname: "{{ memos_service_name }}"
memos_ts_auth_key:

# ==================
#       METUBE
# ==================
# Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)
#   https://github.com/alexta69/metube
# -----
# VERSIONS: https://github.com/alexta69/metube/pkgs/container/metube
# -----
# NOTE 1:
#   * for metube_data_dir '~' is metube_user HOME
#   * for other directories '~' is metube_owner HOME
# -----
metube_managed: false
metube_enabled: false
metube_version:       # Leave blank to leverage bookshelf configuration
metube_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
metube_owner: "{{ ansible_user_id }}"       # <- Who runs the container
metube_user: "{{ metube_owner }}"           # <- The in-container user and data filesystem owner
metube_conf_dir: ~/conf/metube
metube_data_dir: ~/data/Metube
metube_service_name: metube
metube_hostname: "{{ metube_service_name }}"
metube_vhost: "{{ metube_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
metube_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
metube_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars
metube_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/metube.env.j2') }}"
  # References:
  #   * https://github.com/alexta69/metube#%EF%B8%8F-configuration-via-environment-variables
  # ----------
  DEFAULT_THEME=dark

# =====================
#       NAVIDROME
# =====================
# Open source music server and streamer. It gives you freedom to listen to your
# music collection from any browser or mobile device.
#   https://navidrome.org/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/deluan/navidrome/tags
#   * => See tailscale role versions
# -----
# NOTE 1:
#   * for navidrome_data_volumes '~' is navidrome_user HOME
#   * for other directories '~' is navidrome_owner HOME
# -----
navidrome_managed: false
navidrome_enabled: false
navidrome_version:        # Leave blank to leverage bookshelf configuration
navidrome_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
navidrome_owner: "{{ ansible_user_id }}"    # <- Who runs the container
navidrome_user: "{{ navidrome_owner }}"     # <- The in-container user and data filesystem owner
navidrome_conf_dir: ~/conf/navidrome
navidrome_data_volumes:     # <- Volumes for data, NOTE: navidrome needs only to read it
  - ~/data/Music:/music:ro  # (REQUIRED) At least one volume required to be mounted to '/music'
  # - ~/data/Metal:/extra/Metal:ro
  # - ~/data/Pop:/extra/Pop:ro
navidrome_service_name: navidrome
navidrome_hostname: "{{ navidrome_service_name }}"
navidrome_vhost: "{{ navidrome_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
navidrome_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
navidrome_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both navidrome and tailscale.
navidrome_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/navidrome.env.j2') }}"
  # References:
  #   * https://www.navidrome.org/docs/usage/configuration-options/#available-options
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # ND_ENABLEINSIGHTSCOLLECTOR=true
# ----------
# The following settings configure Navidrome to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
navidrome_tailscaled: false
navidrome_ts_version:           # Leave blank to leverage bookshelf configuration
navidrome_ts_service_name: "{{ navidrome_service_name }}-ts"
navidrome_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
navidrome_ts_hostname: "{{ navidrome_service_name }}"
navidrome_ts_auth_key:

# ===============================
#       NGINX-PROXY-MANAGER
# ===============================
# Docker container for managing Nginx proxy hosts with a simple, powerful interface
#   https://nginxproxymanager.com/
# -----
# VERSIONS: https://hub.docker.com/r/jc21/nginx-proxy-manager/tags
# -----
nginx_proxy_manager_managed: false
nginx_proxy_manager_enabled: false
nginx_proxy_manager_version:          # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_manager_owner: "{{ ansible_user_id }}"
nginx_proxy_manager_conf_dir: ~/conf/nginx-proxy-manager
nginx_proxy_manager_service_name: nginx-proxy-manager
nginx_proxy_manager_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_host_net: false   # 'true' allows adding Stream ports without editting compose file
# Port settings only get applied with nginx_proxy_manager_host_net == false
nginx_proxy_manager_web_ui_port:      # Leave blank for bookshelf default, or '-1' to not expose the port
nginx_proxy_manager_http_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
# NOTE: Avoid port conflicts with other reverse proxies
nginx_proxy_manager_ports:            # (OPTIONAL) HOST:CONTAINER ports mapping
  - 80:80                             # <- HTTP
  - 443:443                           # <- HTTPS
nginx_proxy_manager_vhost: "{{ nginx_proxy_manager_service_name }}.domain.local"
# Optional extra env vars
nginx_proxy_manager_extra_env:        # "{{ lookup('template', playbook_dir + '/resources/nginx-proxy-manager.env.j2') }}"
# ----------
# The following settings configure Nginx-Proxy-Manager to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
nginx_proxy_manager_tailscaled: false
nginx_proxy_manager_ts_version:           # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_ts_service_name: "{{ nginx_proxy_manager_service_name }}-ts"
nginx_proxy_manager_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
nginx_proxy_manager_ts_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_ts_auth_key:

# =======================
#       NGINX-PROXY
# =======================
# Automated nginx proxy for Docker containers using docker-gen
#   https://github.com/nginx-proxy/nginx-proxy
# -----
# VERSIONS: https://hub.docker.com/r/nginxproxy/nginx-proxy/tags?name=alpine
# -----
# Exposes:
#   * nginx_proxy_net - proxy network name
#   * nginx_proxy_enabled - Used by other services
# -----
nginx_proxy_managed: false
nginx_proxy_enabled: false
nginx_proxy_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_owner: "{{ ansible_user_id }}"
nginx_proxy_service_name: nginx-proxy
nginx_proxy_hostname: "{{ nginx_proxy_service_name }}"
# NOTE: Avoid port conflicts with other reverse proxies
nginx_proxy_ports:          # (OPTIONAL) HOST:CONTAINER ports mapping
  - 80:80                   # <- HTTP
  - 443:443                 # <- HTTPS
# See:
#   * https://github.com/nginx-proxy/nginx-proxy/blob/main/docs/README.md#ssl-support
#   * https://github.com/nginx-proxy/nginx-proxy/blob/main/docs/README.md#wildcard-certificates
nginx_proxy_cert_bundles:   # (OPTIONAL)
  #   # Demo cert-key generated with:
  #   #
  #   #   openssl req -x509 -days 3650 -noenc -keyout /dev/stdout -out /dev/stdout \
  #   #     -subj "/CN=domain.local" \
  #   #     -addext "subjectAltName=DNS:domain.local,DNS:*.domain.local" \
  #   #   | base64 >./domain.local.bundle.pem.b64
  #   #
  #   # The file is base64 encoded to avoid github quacking for exposed key file
  # - text: "{{ lookup('file', 'domain.local.bundle.pem.b64') | b64decode }}"
  #   # 2 copies of the same file will land in the certs directory
  #   # under domain.local.crt and domain.local.key names
  #   domain: domain.local
# Optional extra env vars
nginx_proxy_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/nginx-proxy.env.j2') }}"
  # References:
  #   * https://github.com/nginx-proxy/nginx-proxy/blob/main/docs/README.md#proxy-container
  # ----------
  # HTTPS_METHOD=noredirect

# ====================
#       OLIVETIN
# ====================
# Safe and simple access to predefined shell commands from a web interface.
#   https://olivetin.app/
#   https://docs.olivetin.app/
# -----
# VERSIONS: https://hub.docker.com/r/jamesread/olivetin/tags
# -----
olivetin_managed: false
olivetin_enabled: false
olivetin_version:       # Leave blank to leverage bookshelf configuration
olivetin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
olivetin_owner: "{{ ansible_user_id }}"
olivetin_conf_dir: ~/conf/olivetin
olivetin_service_name: olivetin
olivetin_hostname: "{{ olivetin_service_name }}"
olivetin_vhost: "{{ olivetin_service_name }}.domain.local"
olivetin_web_ui_port:   # Leave blank to leverage bookshelf configuration
# Scripts to be uploaded to 'name' filename under "{{ olivetin_compose_dir }}/scripts"
# directory, which is read only mounted mounted to the container '/app/scripts'.
# See files/sample.sh for a demo.
olivetin_scripts:
#   - text: "{{ lookup('file', 'sample.sh') }}"
#     name: s1.sh   # /app/scripts/s1.sh in the container
#     mode: '0755'  # <- Optional. Defaults to '0755'
#   - text: "{{ lookup('template', playbook_dir + 'resources/olivetin/script2.sh.j2') }}"
#     name: s2.sh
# Extra mounts, can be used to access docker host resources
olivetin_volumes:       # ['~:/app/home:ro', '/etc:/app/etc:ro']
# Custom config. See the default one in files/config.yaml.
# templates/config.base.yaml.j2 is always appended in the end.
olivetin_config:        # "{{ lookup('file', 'config.yaml') }}"

# ====================
#       PAIRDROP
# ====================
# Transfer Files Cross-Platform. No Setup, No Signup.
#   https://github.com/schlagmichdoch/PairDrop
# ALTERNATIVE (when time comes);
#   https://github.com/localsend/localsend
#   https://github.com/localsend/web
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/pairdrop/tags?name=-ls
#   * => See tailscale role versions
# -----
pairdrop_managed: false
pairdrop_enabled: false
pairdrop_version:       # Leave blank to leverage bookshelf configuration
pairdrop_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
pairdrop_owner: "{{ ansible_user_id }}"
pairdrop_conf_dir: ~/conf/pairdrop
pairdrop_service_name: pairdrop
pairdrop_hostname: "{{ pairdrop_service_name }}"
pairdrop_vhost: "{{ pairdrop_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
pairdrop_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
pairdrop_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both PairDrop and tailscale.
pairdrop_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/pairdrop.env.j2') }}"
  # References:
  #   * https://hub.docker.com/r/linuxserver/pairdrop#parameters
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # RATE_LIMIT=true
# ----------
# The following settings configure PairDrop to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
pairdrop_tailscaled: false
pairdrop_ts_version:          # Leave blank to leverage bookshelf configuration
pairdrop_ts_service_name: "{{ pairdrop_service_name }}-ts"
pairdrop_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
pairdrop_ts_hostname: "{{ pairdrop_service_name }}"
pairdrop_ts_auth_key:

# ===========================
#       PORTAINER-AGENT
# ===========================
# The Portainer agent.
#   https://www.portainer.io/
# -----
# VERSIONS: https://hub.docker.com/r/portainer/agent/tags?name=alpine
# -----
portainer_agent_managed: false
portainer_agent_enabled: false
portainer_agent_version:        # Leave blank to leverage bookshelf configuration
portainer_agent_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_agent_conf_dir: ~/conf/portainer-agent
portainer_agent_owner: "{{ ansible_user_id }}"
portainer_agent_service_name: portainer-agent
portainer_agent_hostname: "{{ portainer_agent_service_name }}"
portainer_agent_host_management: false
# Can disable ports if plan to access only via tailnet or reverse-proxy
portainer_agent_tcp_port:       # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Portainer agent and Tailscale.
portainer_agent_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/portainer-agent.env.j2') }}"
# ----------
# The following settings configure Portainer agent to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
portainer_agent_tailscaled: false
portainer_agent_ts_version:     # Leave blank to leverage bookshelf configuration
portainer_agent_ts_service_name: "{{ portainer_agent_service_name }}-ts"
# Hostname within tailnet
portainer_agent_ts_hostname: "{{ portainer_agent_service_name }}"
portainer_agent_ts_auth_key:

# =====================
#       PORTAINER
# =====================
# A universal container management platform.
#   https://www.portainer.io/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/portainer/portainer-ce/tags?name=alpine
#   * => See tailscale role versions
# -----
portainer_managed: false
portainer_enabled: false
portainer_version:        # Leave blank to leverage bookshelf configuration
portainer_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_owner: "{{ ansible_user_id }}"
portainer_conf_dir: ~/conf/portainer
portainer_service_name: portainer
portainer_hostname: "{{ portainer_service_name }}"
portainer_vhost: "{{ portainer_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
portainer_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
# Expose port for agents
portainer_expose_ssh_tunnel_port: false
# Optional extra env vars, will affect both Portainer and Tailscale.
portainer_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/portainer.env.j2') }}"
# ----------
# The following settings configure Portainer to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
portainer_tailscaled: false
portainer_ts_version:          # Leave blank to leverage bookshelf configuration
portainer_ts_service_name: "{{ portainer_service_name }}-ts"
portainer_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
portainer_ts_hostname: "{{ portainer_service_name }}"
portainer_ts_auth_key:

# =======================
#       QBITTORRENT
# =======================
# Free and reliable P2P Bittorrent client
#   https://www.qbittorrent.org/
#   https://hub.docker.com/r/linuxserver/qbittorrent
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/qbittorrent/tags?name=-ls
#   * https://github.com/VueTorrent/VueTorrent/pkgs/container/vuetorrent-lsio-mod/versions
# -----
# NOTE 1:
#   * for qbittorrent_data_volumes '~' is qbittorrent_user HOME
#   * for other directories '~' is qbittorrent_owner HOME
#
# NOTE 2: Temporary password for the admin user will be printed to the container
# log on startup
#
# NOTE 3: Switch to alternative UI
#   Settings > WebUI > check Use alternative WebUI, Files location: /vuetorrent
# -----
qbittorrent_managed: false
qbittorrent_enabled: false
qbittorrent_version:              # Leave blank to leverage bookshelf configuration
qbittorrent_vuetorrent_version:   # Leave blank to leverage bookshelf configuration
qbittorrent_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
qbittorrent_owner: "{{ ansible_user_id }}"    # <- Who runs the container
qbittorrent_user: "{{ qbittorrent_owner }}"   # <- The in-container user and data filesystem owner
qbittorrent_conf_dir: ~/conf/qbittorrent
qbittorrent_data_volumes:       # <- Volumes for data
  - ~/data/Torrent:/downloads   # (REQUIRED) At least one volume required to be mounted to '/downloads'
  # - ~/data/Movies/Torrent:/extra/Movies
  # - ~/data/Music/Torrent:/extra/Music
qbittorrent_service_name: qbittorrent
qbittorrent_hostname: "{{ qbittorrent_service_name }}"
qbittorrent_vhost: "{{ qbittorrent_service_name }}.domain.local"
qbittorrent_web_ui_port:          # Leave blank to leverage bookshelf configuration
qbittorrent_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional. Configure notifications script, that will be available in the container
#   /scripts/notify.sh
# Configure in qBittorrent UI -> Settings -> Downloads -> External ... on finished
# Leave blank to skip configuration. If not blank, rest notify_* should be
# configured accordingly
qbittorrent_notify_provider:  # discord|gotify|telegram
qbittorrent_notify_token:     # "{{ lookup('file', playbook_dir + '/resources/qbittorrent.secret.sh') }}"
qbittorrent_notify_server:    # (REQUIRED for gotify)
qbittorrent_notify_chat_id:   # (REQUIRED for telegram)
# Optional extra env vars
qbittorrent_extra_env:        # "{{ lookup('template', playbook_dir + '/resources/qbittorrent.env.j2') }}"

# ========================
#       SILVERBULLET
# ========================
# An open source personal productivity platform built on Markdown, turbo
# charged with the scripting power of Lua
#   https://silverbullet.md/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/zefhemel/silverbullet/tags
# -----
silverbullet_managed: false
silverbullet_enabled: false
silverbullet_version:       # Leave blank to leverage bookshelf configuration
silverbullet_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
silverbullet_owner: "{{ ansible_user_id }}"   # <- Who runs the container
silverbullet_conf_dir: ~/conf/silverbullet
silverbullet_service_name: silverbullet
silverbullet_hostname: "{{ silverbullet_service_name }}"
silverbullet_vhost: "{{ silverbullet_service_name }}.domain.local"
silverbullet_web_ui_port:   # Leave blank to leverage bookshelf configuration
# Auth
silverbullet_auth_user:     # (OPTIONAL)
silverbullet_auth_pass:     # (REQUIRED when non-empty user) If user not set, user defaults to 'admin'
# silverbullet_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional extra env vars
silverbullet_extra_env:     # "{{ lookup('template', playbook_dir + '/resources/silverbullet.env.j2') }}"

# ====================
#       SMTP4DEV
# ====================
# Fake smtp email server for development and testing.
#   https://github.com/rnwood/smtp4dev
# -----
# VERSIONS:
#   * https://hub.docker.com/r/rnwood/smtp4dev/tags?name=v
#   * => See tailscale role versions
# Configuration:
#   * https://github.com/rnwood/smtp4dev/blob/master/docs/Configuration.md
#   * https://github.com/rnwood/smtp4dev/blob/master/Rnwood.Smtp4dev/appsettings.json
# -----
# Exposes:
#   * smtp4dev_smtp_port_ext - to be used by other tools
#   * smtp4dev_imap_port_ext - to be used by other tools
# -----
smtp4dev_managed: false
smtp4dev_enabled: false
smtp4dev_version:       # Leave blank to leverage bookshelf configuration
smtp4dev_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
smtp4dev_owner: "{{ ansible_user_id }}"
smtp4dev_conf_dir: ~/conf/smtp4dev
smtp4dev_service_name: smtp4dev
smtp4dev_hostname: "{{ smtp4dev_service_name }}"
smtp4dev_vhost: "{{ smtp4dev_service_name }}.domain.local"
smtp4dev_web_ui_port:   # Leave blank to leverage bookshelf configuration
smtp4dev_smtp_port:     # Leave blank to leverage bookshelf configuration
smtp4dev_imap_port:     # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both smtp4dev and tailscale.
smtp4dev_extra_env:     # "{{ lookup('template', playbook_dir + '/resources/smtp4dev.env.j2') }}"
# ----------
# The following settings configure smtp4dev to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
smtp4dev_tailscaled: false
smtp4dev_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
smtp4dev_tailscale_ts_hostname: "{{ smtp4dev_service_name }}-tailscale"
smtp4dev_tailscale_auth_key:

# =====================
#       SYNCTHING
# =====================
# Open Source Continuous File Synchronization
#   https://syncthing.net/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/syncthing/tags?name=-ls
#   * => See tailscale role versions
# -----
# NOTE 1:
#   * for syncthing_data_volumes  '~' is syncthing_user HOME
#   * for other directories '~' is syncthing_owner HOME
# -----
syncthing_managed: false
syncthing_enabled: false
syncthing_version:              # Leave blank to leverage bookshelf configuration
syncthing_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
syncthing_owner: "{{ ansible_user_id }}"  # <- Who runs the container
syncthing_user: "{{ syncthing_owner }}"   # <- The in-container user and data filesystem owner
syncthing_conf_dir: ~/conf/syncthing
syncthing_data_volumes:   # <- Volumes for data
  - ~/data:/data          # (REQUIRED) At least one volume required
  # - ~/data2:/extra/data2
  # - ~/data3:/extra/data3
syncthing_service_name: syncthing
syncthing_hostname: "{{ syncthing_service_name }}"
syncthing_vhost: "{{ syncthing_service_name }}.domain.local"
syncthing_web_ui_port:          # Leave blank to leverage bookshelf configuration
syncthing_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional extra env vars, will affect both syncthing and tailscale.
syncthing_extra_env:            # "{{ lookup('template', playbook_dir + '/resources/syncthing.env.j2') }}"
# ----------
# The following settings configure Syncthing to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
syncthing_tailscaled: false
syncthing_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
syncthing_tailscale_ts_hostname: "{{ syncthing_service_name }}-tailscale"
syncthing_tailscale_auth_key:

# =====================
#       TAILSCALE
# =====================
# Tailscale makes creating software-defined networks easy: securely connecting
# users, services, and devices.
#   https://tailscale.com/
# -----
# VERSIONS: https://hub.docker.com/r/tailscale/tailscale/tags
# -----
# Meant primarily for tailscale as a VPN gateway to a local network
#
# IMPORTANT 1: Auth key is required for initial container run:
# https://tailscale.com/admin/settings/keys -> 'Generate auth key...'
#   -> generate single time or reusable
#
# IMPORTANT 2: After deployment https://tailscale.com/admin/machines -> 3 dots
# next to the machine -> Disable key expiry (to avoid re-login to tailscale)
#
# IMPORTANT 3: With changed routes or exit node params service requires a valid
# auth key, i.e. if it's one-timer new one must be provided
#
# IMPORTANT 4: With version update also update auth key (see IMPORTANT 1)
#
# NOTE 1: The service is useless unless it's configured for exit node or subnet
# router. Both need to be confirmed in the machines admin
# -----
tailscale_managed: false
tailscale_enabled: false
tailscale_version:          # Leave blank to leverage bookshelf configuration
tailscale_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
tailscale_owner: "{{ ansible_user_id }}"
tailscale_conf_dir: ~/conf/tailscale
tailscale_service_name: tailscale
tailscale_hostname: "{{ tailscale_service_name }}"
# Hostname within tailnet
tailscale_ts_hostname: "{{ tailscale_service_name }}"
tailscale_auth_key:         # (REQUIRED)
tailscale_routes:           # '192.168.0.0/24,192.168.2.0/24'
tailscale_exit_node: false  # Advertise exit node
tailscale_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars
tailscale_extra_env: |      # "{{ lookup('file', playbook_dir + '/resources/tailscale.env') }}"
  # These are used in demo gatus.sh healthcheck script.
  #   https://github.com/TwiN/gatus/#external-endpoints
  # ----------
  GATUS_URL=https://gatus.domain.local
  GATUS_KEY=bookshelf_tailscale   # {GROUP}_{ENDPOINT}
tailscale_secrets_env: |     # "{{ lookup('file', playbook_dir + '/resources/tailscale.secret.env') }}"
  # Same as extra env, but for secrets
  # ----------
  GATUS_TOKEN=changeme
# ----------
# The following settings configure healthcheck reporter sidecar container.
# The container will start if tailscale_health_script not empty.
# The container is equipped with curl, wget and bash.
tailscale_health_script:           # "{{ lookup('file', 'gatus.sh') }}"
tailscale_health_alpine_version:   # Leave blank to leverage bookshelf configuration
tailscale_health_service_name: "{{ tailscale_service_name }}-health"
tailscale_health_check_schedule: '*/1 * * * *'   # <- Every minute

# ======================
#       TORRSERVER
# ======================
# Torrent stream server
#   https://github.com/YouROK/TorrServer
# -----
# VERSIONS:
#   * https://github.com/yourok/TorrServer/pkgs/container/torrserver
#   * => See tailscale role versions
# -----
torrserver_managed: false
torrserver_enabled: false
torrserver_version:            # Leave blank to leverage bookshelf configuration
torrserver_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
torrserver_owner: "{{ ansible_user_id }}"
torrserver_conf_dir: ~/conf/torrserver
torrserver_service_name: torrserver
torrserver_hostname: "{{ torrserver_service_name }}"
torrserver_vhost: "{{ torrserver_service_name }}.domain.local"
torrserver_web_ui_port:        # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both TorrServer and Tailscale.
torrserver_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/torrserver.env.j2') }}"
  # References:
  #   * https://github.com/YouROK/TorrServer/#environments
  # ----------
  # Auth seems to break the server
  # TS_HTTPAUTH=1   # <- Enable auth. https://github.com/YouROK/TorrServer/#authentication
# ----------
# Disable if plan to access only via tailnet or reverse-proxy
torrserver_web_ui_ports_exposed: true
# ----------
# The following settings configure TorrServer to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
torrserver_tailscaled: false
torrserver_ts_version:   # Leave blank to leverage bookshelf configuration
torrserver_ts_service_name: "{{ torrserver_service_name }}-ts"
# Hostname within tailnet
torrserver_ts_hostname: "{{ torrserver_service_name }}"
torrserver_ts_auth_key:

# ====================
#       TWINGATE
# ====================
# It's time to ditch your VPN
#   https://www.twingate.com/
# -----
# VERSIONS: https://hub.docker.com/r/twingate/connector/tags
# -----
twingate_managed: false
twingate_enabled: false
twingate_version:         # Leave blank to leverage bookshelf configuration
twingate_compose_dir:     # Unless you know ..., leave blank to leverage bookshelf configuration
twingate_owner: "{{ ansible_user_id }}"
twingate_service_name: twingate
twingate_hostname: "{{ twingate_service_name }}"
twingate_tenant_name:     # (REQUIRED) Your twingate network name
# https://<tenant>.twingate.com/networks/overview -> 'Deploy ... Connector'
#   -> 'Docker' -> 'Generate Tokens'
twingate_access_token:    # (REQUIRED)
twingate_refresh_token:   # (REQUIRED)
# Optional extra env vars
twingate_extra_env:       # "{{ lookup('file', playbook_dir + '/resources/twingate.env') }}"

# =======================
#       UPTIME-KUMA
# =======================
# A fancy self-hosted monitoring tool
#   https://github.com/louislam/uptime-kuma
# -----
# VERSIONS:
#   * https://hub.docker.com/r/louislam/uptime-kuma/tags
#   * => See tailscale role versions
# -----
uptime_kuma_managed: false
uptime_kuma_enabled: false
uptime_kuma_version:        # Leave blank to leverage bookshelf configuration
uptime_kuma_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
uptime_kuma_owner: "{{ ansible_user_id }}"
uptime_kuma_conf_dir: ~/conf/uptime-kuma
uptime_kuma_service_name: uptime-kuma
uptime_kuma_hostname: "{{ uptime_kuma_service_name }}"
uptime_kuma_vhost: "{{ uptime_kuma_service_name }}.domain.local"
uptime_kuma_web_ui_port:    # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both Uptime Kuma and tailscale.
uptime_kuma_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/uptime-kuma.env.j2') }}"
# ----------
# The following settings configure Uptime Kuma to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
uptime_kuma_tailscaled: false
uptime_kuma_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
uptime_kuma_tailscale_ts_hostname: "{{ uptime_kuma_service_name }}-tailscale"
uptime_kuma_tailscale_auth_key:

# =======================
#       VAULTWARDEN
# =======================
# The community driven password manager.
#   https://www.vaultwarden.net/
# -----
# VERSIONS: https://hub.docker.com/r/vaultwarden/server/tags?name=alpine
# -----
vaultwarden_managed: false
vaultwarden_enabled: false
vaultwarden_version:        # Leave blank to leverage bookshelf configuration
vaultwarden_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
vaultwarden_owner: "{{ ansible_user_id }}"
vaultwarden_conf_dir: ~/conf/vaultwarden
vaultwarden_service_name: vaultwarden
vaultwarden_hostname: "{{ vaultwarden_service_name }}"
vaultwarden_vhost: "{{ vaultwarden_service_name }}.domain.local"
# Required with https via reverse proxy, vaultwarden needs to know it's https
# to work properly with attachments.
# https://github.com/dani-garcia/vaultwarden/wiki/Using-Docker-Compose
# Ex.: https://{{ vaultwarden_vhost }}
vaultwarden_vdomain:
# Access token for http(s)://VAULTWARDEN_HOST/admin
# Generate with:
#   docker container run -it --rm vaultwarden/server:latest /vaultwarden hash
vaultwarden_admin_token_hash:
vaultwarden_web_ui_port:    # Leave blank to leverage bookshelf configuration
vaultwarden_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# https://github.com/dani-garcia/vaultwarden/blob/main/.env.template
# Optional extra env vars
vaultwarden_extra_env:      # "{{ lookup('file', playbook_dir + '/resources/vaultwarden.env') }}"

# ===================
#       VIKUNJA
# ===================
# The to-do app to organize your life.
#   https://vikunja.io/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/vikunja/vikunja/tags
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# Options:
#   * https://vikunja.io/docs/config-options/
# -----
vikunja_managed: false
vikunja_enabled: false
vikunja_version:            # Leave blank to leverage bookshelf configuration
vikunja_postgres_version:   # Leave blank to leverage bookshelf configuration
vikunja_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
vikunja_owner: "{{ ansible_user_id }}"
vikunja_conf_dir: ~/conf/vikunja
vikunja_service_name: vikunja
vikunja_hostname: "{{ vikunja_service_name }}"
vikunja_vhost: "{{ vikunja_service_name }}.domain.local"
vikunja_web_ui_port:        # Leave blank to leverage bookshelf configuration
vikunja_tz: "{{ ansible_date_time.tz | default('UTC') }}"
vikunja_db_pass:            # (REQUIRED) Something hard and random
vikunja_jwtsecret:          # (REQUIRED) Something hard and random
# Optional extra env vars, will affect both Vikunja and Tailscale.
vikunja_extra_env:          # "{{ lookup('template', playbook_dir + '/resources/vikunja.env.j2') }}"
# ----------
# The following settings configure Vikunja to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
vikunja_tailscaled: false
vikunja_tailscale_version:  # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
vikunja_tailscale_ts_hostname: "{{ vikunja_service_name }}-tailscale"
vikunja_tailscale_auth_key:

# ===================
#       WG-EASY
# ===================
# The easiest way to run WireGuard VPN + Web-based Admin UI.
#   https://github.com/wg-easy/wg-easy
# -----
# Versions:
# * https://github.com/wg-easy/wg-easy/pkgs/container/wg-easy/versions?filters%5Bversion_type%5D=tagged
# Options description:
# * https://github.com/wg-easy/wg-easy/tree/production?tab=readme-ov-file#options
# * https://github.com/wg-easy/wg-easy/blob/production/How_to_generate_an_bcrypt_hash.md
# -----
wg_easy_managed: false
wg_easy_enabled: false
wg_easy_version:        # Leave blank to leverage bookshelf configuration
wg_easy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
wg_easy_owner: "{{ ansible_user_id }}"
wg_easy_conf_dir: ~/conf/wg-easy
wg_easy_service_name: wg-easy
wg_easy_hostname: "{{ wg_easy_service_name }}"
wg_easy_vhost: "{{ wg_easy_service_name }}.domain.local"
wg_easy_web_ui_port:    # Leave blank to leverage bookshelf configuration
wg_easy_udp_port: 51820   # Must match the UDP port configured in Admin panel -> Config
wg_easy_insecure: 'false' # Allow login via plain http
# Optional extra env vars
wg_easy_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/wg-easy.env.j2') }}"

# ==================
#       WIKIJS
# ==================
# A modern and powerful wiki app built on Node.js
#   https://js.wiki/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/wikijs/tags?name=-ls
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
wikijs_managed: false
wikijs_enabled: false
wikijs_version:             # Leave blank to leverage bookshelf configuration
wikijs_postgres_version:    # Leave blank to leverage bookshelf configuration
wikijs_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
wikijs_owner: "{{ ansible_user_id }}"
wikijs_conf_dir: ~/conf/wikijs
wikijs_service_name: wikijs
wikijs_hostname: "{{ wikijs_service_name }}"
wikijs_vhost: "{{ wikijs_service_name }}.domain.local"
wikijs_web_ui_port:         # Leave blank to leverage bookshelf configuration
wikijs_db_pass:             # (REQUIRED) Something hard and random
wikijs_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional extra env vars, will affect both Wiki.js and tailscale.
wikijs_extra_env:           # "{{ lookup('template', playbook_dir + '/resources/wikijs.env.j2') }}"
# ----------
# The following settings configure Wiki.js to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
wikijs_tailscaled: false
wikijs_tailscale_version:   # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
wikijs_tailscale_ts_hostname: "{{ wikijs_service_name }}-tailscale"
wikijs_tailscale_auth_key:



###########################
###       DESKTOP       ###
###########################

# ====================
#       AUDACITY
# ====================
# Free Audio editor, recorder, music making and more!
#   https://www.audacityteam.org/
# -----
audacity_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
audacity_install_method: flatpak

# =================
#       BRAVE
# =================
# The browser that puts you first. Block ads. Save data. And get way faster
# webpages. Just by switching your browser.
#   https://brave.com/
# -----
brave_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
brave_install_method: flatpak

# ==================
#       CHROME
# ==================
# Official web browser from Google, built to be fast, secure, and customizable.
#   https://www.google.com/chrome/
# -----
chrome_managed: false

# =================
#       COPYQ
# =================
# Advanced clipboard manager with editing and scripting features.
#   https://hluk.github.io/CopyQ/
# -----
copyq_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * pm
# * flatpak   # <- Forced for non-Ubuntu-like
copyq_install_method: pm
copyq_starters:         # <- Existing users to have copyq autostarted
  # - "{{ ansible_user_id }}"

# ===================
#       DBEAVER
# ===================
# Universal Database Tool.
#   https://dbeaver.io/
# -----
dbeaver_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
dbeaver_install_method: flatpak

# =====================
#       DOUBLECMD
# =====================
# Cross platform open source file manager with two panels side by side.
#   https://doublecmd.sourceforge.io/
# -----
doublecmd_managed: false    # "{{ factum_os_family in ['debian'] }}"  # <- Only supported by Debian family
doublecmd_ui_lib: gtk       # <- 'gtk' for Gnome or 'qt' for KDE
doublecmd_starters:         # <- Existing users to have doublecmd autostarted
  # - "{{ ansible_user_id }}"

# ===================
#       FLATPAK
# ===================
flatpak_managed: false
flatpak_repos:    # <- Currently only the ones in the example are supported
  # - flathub

# ================
#       GIMP
# ================
# GNU Image Manipulation Program
#   https://www.gimp.org/
# -----
gimp_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
gimp_install_method: flatpak

# ======================
#       INPUT-LEAP
# ======================
# Open-source KVM software (Barrier fork)
#   https://github.com/input-leap/input-leap
# -----
input_leap_managed: false
input_leap_starters:  # <- Existing users to have InputLeap autostarted
  # - "{{ ansible_user_id }}"

# =========================
#       INTELLIJ-IDEA
# =========================
# The IDE for Java and Kotlin
#   https://www.jetbrains.com/idea/
# -----
intellij_idea_managed: false
# Ultimate edition, unless community
intellij_idea_ultimate: false
# Supported values in prefered order (due to available CLI interface):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
intellij_idea_install_method: snap

# ===========================
#       JELLYFIN-PLAYER
# ===========================
# Desktop client for Jellyfin media server.
#   https://github.com/jellyfin/jellyfin-media-player
# -----
jellyfin_player_managed: false
jellyfin_player_starters:   # <- Existing users to have jellyfin player autostarted
  # - "{{ ansible_user_id }}"

# ==========================
#       JOPLIN-DESKTOP
# ==========================
# Open source note-taking app. Capture your thoughts and securely access them from any device.
#   https://joplinapp.org/
# -----
joplin_desktop_managed: false
# Supported values in prefered order (due newer versions in snap):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
joplin_desktop_install_method: snap

# ====================
#       KDENLIVE
# ====================
# FOS Video Editor.
#   https://kdenlive.org/
# -----
kdenlive_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
kdenlive_install_method: flatpak

# ======================
#       MKVTOOLNIX
# ======================
# Ultimate Tool for Editing and Merging MKV Files.
#   https://mkvtoolnix.org/
# -----
mkvtoolnix_managed: false
# Supported values in prefered order (due to available CLI interface):
# * pm
# * flatpak   # <- Forced for non-Debian like
mkvtoolnix_install_method: pm

# ====================
#       NXPLAYER
# ====================
# NoMachine client.
#   https://www.nomachine.com/
# -----
nxplayer_managed: false

# ======================
#       OBS-STUDIO
# ======================
# Free and open source software for video recording and live streaming.
#   https://obsproject.com/
# -----
obs_studio_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * pm
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
obs_studio_install_method: pm

# =================
#       OPERA
# =================
# Experience faster, distraction-free browsing with Ad blocking, and browse privately.
#   https://www.opera.com/
# -----
opera_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
opera_install_method: flatpak

# ====================
#       PHPSTORM
# ====================
# PHP IDE for Professional Development
#   https://www.jetbrains.com/phpstorm/
# -----
phpstorm_managed: false
# Supported values in prefered order (due to available CLI interface):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
phpstorm_install_method: snap

# =================
#       PINTA
# =================
# FOS painting and image editing program designed for simplicity and power.
#   https://www.pinta-project.com/
# -----
pinta_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
pinta_install_method: flatpak

# ===================
#       POSTMAN
# ===================
# Platform for building and using APIs
#   https://www.postman.com/
# -----
postman_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
postman_install_method: flatpak

# ===================
#       SUBLIME
# ===================
# Text Editing, Done Right.
#   https://www.sublimetext.com/
# -----
sublime_managed: false
sublime_merge: false    # <- Install sublime-merge tool
# Supported values in prefered order (due to available CLI interface):
# * pm    # <- Forced for non-Ubuntu-like
# * snap
sublime_install_method: pm

# =================
#       TEAMS
# =================
teams_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
teams_install_method: flatpak
teams_starters:   # <- Existing users to have teams autostarted
  # - "{{ ansible_user_id }}"

# ====================
#       TELEGRAM
# ====================
# A new era of messaging.
#   https://telegram.org/
# -----
telegram_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
telegram_install_method: flatpak
telegram_starters:         # <- Existing users to have telegram autostarted
  # - "{{ ansible_user_id }}"

# ======================
#       TERMINATOR
# ======================
terminator_managed: false

# =================
#       VIBER
# =================
viber_managed: false
viber_starters:   # <- Existing users to have viber autostarted
  # - "{{ ansible_user_id }}"

# ===============
#       VLC
# ===============
vlc_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
vlc_install_method: flatpak

# ==================
#       VSCODE
# ==================
vscode_managed: false
# Supported values in prefered order (due to available CLI interface):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
vscode_install_method: snap
vscode_extensions:      # <- Per user extensions
  # - owner: "{{ ansible_user_id }}"  # <- Existing user. Must be unique
  #   extensions:
  #     - editorconfig.editorconfig
  #     - mads-hartmann.bash-ide-vscode
  #     - timonwong.shellcheck
  #     - foxundermoon.shell-format
  #     - redhat.ansible

# ===================
#       WHATSIE
# ===================
# Feature rich WhatsApp Client for Desktop Linux.
#   https://github.com/keshavbhatt/whatsie
# -----
whatsie_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
whatsie_install_method: flatpak



#######################
###       DEV       ###
#######################

# ==================
#       GOLANG
# ==================
golang_managed: false   # "{{ factum_os_like in ['alpine', 'ubuntu'] }}"  # <- Only supported

# ==================
#       NODEJS
# ==================
# NOTE:
#   For Ubuntu see notes in snap role
# ----------
nodejs_managed: false   # "{{ factum_os_like in ['ubuntu', 'alpine'] }}"  # <- Only supported by these
# Versions (better pick latest LTS) and channels lists:
#   * https://nodejs.org/en/about/previous-releases#looking-for-the-latest-release-of-a-version-branch
#   * https://snapcraft.io/node
nodejs_snap_channel:    # Only applicable for Ubuntu. Leave blank to leverage bookshelf defaults
