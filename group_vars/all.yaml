---
#
# THIS IS A DEMO CONFIGURATION MOSTLY FOR TESTING
#

# *_managed vars denote whether ansible takes care of the application or not.
#   `X_managed: false` application will still fire if it is in `Y_managed: true`
#   application dependencies
#
# *_enabled vars denote whether the application is enabled or not. Unlike
#   *_managed it disables the application when `*_enabled: false`

service_port:
  nginx_proxy_http: "{{ (nginx_proxy_manager_enabled | default(false)) | ternary('9080', '80') }}"

###
### Server connection configuration
###
# Don't keep passwords in plain text, encrypt them. One option would be:
#   ansible-vault encrypt_string -J PASSWORD_PLACEHOLDER
ansible_user: "{{ lookup('env', 'ANSIBLE_USER') }}"
ansible_password: "{{ lookup('env', 'ANSIBLE_USER_PASS') }}"
ansible_become_password: "{{ lookup('env', 'ANSIBLE_USER_PASS') }}"

#################### {{ ROLES_CONF_TS4LE64m91 }} ####################


################
###   BASE   ###
################

# ===============
#       AGE
# ===============
# Versions: https://github.com/FiloSottile/age/releases
# -----
age_managed: true
age_version:    # Leave blank to leverage bookshelf configuration

# ================
#       BASH
# ================
bash_managed: true
bash_completion: true
bash_users:   # <- Existing users to use bash by default
  - "{{ ansible_user_id }}"

# ==========
#   DOCKER
# ==========
docker_managed: true
docker_users:   # <- Users to be added to docker group
  - "{{ ansible_user_id }}"

# =========
#   ENVAR
# =========
envar_managed: true
envar_users:  # <- Existing users to have access to envar
  - "{{ ansible_user_id }}"
  - root

# ===============
#       FZF
# ===============
# Versions: https://github.com/junegunn/fzf/releases
# -----
fzf_managed: true
fzf_version:    # Leave blank to leverage bookshelf configuration
fzf_users:  # <- Existing users to have access to fzf
  - "{{ ansible_user | default(ansible_user_id) }}"
  - root

# =======
#   GIT
# =======
# git_extraconf:
#   - owner: "{{ ansible_user_id }}"    # <- Required, must be unique
#     template: gitconfig.extra.ini.j2  # <- Required. No full path for the ones defined in the role
#     # Optional variables, can be addressed in the template via 'conf' map
#     user_name: Foo            # <- 'conf.user_name' in the template
#     user_email: foo@bar.baz
#   - owner: another-user
#     template: "{{ playbook_dir }}/resources/git/extra.ini.j2" # <- Full path for ones out of the role directory
#     # ...
git_managed: true
git_extraconf:
  - owner: "{{ ansible_user_id }}"
    template: gitconfig.extra.ini.j2
    user_name: Foo
    user_email: foo@bar.baz
  # - owner: root
  #   template: "{{ playbook_dir }}/resources/git/extra.ini.j2"

# ===========
#   PS1-GIT
# ===========
ps1_git_managed: true
ps1_git_users:  # <- Existing users to have this PS1
  - "{{ ansible_user_id }}"
  - root

# =========
#   SNAPD
# =========
# Not supported by Alpine
# -----
# snapd_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines
# -----
snapd_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines

# ============================
#       TAILSCALE-CLIENT
# ============================
tailscale_client_managed: true

# ==============
#      TMUX
# ==============
tmux_managed: true
tmux_sane_users:   # <- Existing users to source my preferred configuration
  - "{{ ansible_user_id }}"

# ===============
#      TMUXP
# ===============
# Not supported by Alpine
# -----
# tmuxp_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines
# -----
# tmuxp_conf:
#   - owner: "{{ ansible_user_id }}"
#     conf:
#         # Also available: triple.yaml.j2, quad.yaml.j2
#       - template: sensors.yaml.j2   # <- Required. No full path for the ones defined in the role
#         name: my-sensors            # <- Required. Will land in ~/.tmuxp/my-sensors.yaml
#         # Optional variables, can be addressed in the template via 'conf' map
#         session_name: Foo           # <- 'conf.session_name' in the template
#         window_name: bar
#       - template: "{{ playbook_dir }}/resources/tmuxp/demo1.yaml.j2"  # <- Full path for ones out of the role directory
#         ...
#   - owner: ...
#     ...
tmuxp_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- All hosts but Alpines
tmuxp_conf:
  - owner: "{{ ansible_user_id }}"
    conf:
        # Also available: triple.yaml.j2, quad.yaml.j2
      - template: sensors.yaml.j2
        name: sensors
        # Vars
        session_name: Sensors
        window_name: sensors
      - template: triple.yaml.j2
        name: triple
        pane3_start_dir: ~/tmp

# ====================
#       TOOLBELT
# ====================
# Contains very basic and often needed tools that for most often usage don't
# require any additional configuration. Some of the tools are categorized and
# the categories can be used to install the whole set of underlying tools.
# -----
# * editors:      nano, neovim, vim
# * downloaders:  curl, wget
# * viewers:      bat, glow
# * htop, jq, sensors, skate, speedtest, tar, wishlist
# -----
# toolbelt_pick:    # <- Install only a fraction. Leave empty to install all
#   - nano            # <- nano will be installed
#   - downloaders     # <- All downloaders will be installed
#   - tar             # <- tar will be installed
# toolbelt_exclude: # <- Opposed to toolbelt_pick, higher priority than toolbelt_pick
#   - wget            # <- In conjunction with toolbelt_pick above, only curl will be installed
#   - editors         # <- Dispite toolbelt_pick above, nano will not be installed
toolbelt_managed: true
toolbelt_pick:
toolbelt_exclude:

# ========================
#       VIRT-MANAGER
# ========================
# Virtual Machine Manager
# -----
# Only supports Debian-like hosts
# -----
# virt_manager_managed: "{{ factum_os_family not in ['debian'] }}"
# virt_manager_users:   # <- Existing users to be added to libvirt group
#   - "{{ ansible_user_id }}"
virt_manager_managed: "{{ factum_os_family in ['debian'] }}"
virt_manager_users:   # <- Existing users to be added to libvirt group
  - "{{ ansible_user_id }}"



###################
###   SERVICE   ###
###################

# ===================
#       ADGUARD
# ===================
# Versions:
# * https://hub.docker.com/r/adguard/adguardhome/tags
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
adguard_managed: "{{ factum_os_like not in ['ubuntu'] }}"   # <- All hosts but Ubuntu
adguard_enabled: true
adguard_version:        # Leave blank to leverage bookshelf configuration
adguard_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
adguard_owner: "{{ ansible_user_id }}"
adguard_conf_dir: ~/conf/adguard
adguard_service_name: adguard
adguard_hostname: "{{ adguard_service_name }}"
adguard_vhost: "{{ adguard_service_name }}.domain.local"
adguard_web_ui_port:    # Leave blank to leverage bookshelf configuration
adguard_init_ui_port:   # Leave blank to leverage bookshelf configuration
adguard_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file, will affect both adguard
# and tailscale. Ex.: "{{ playbook_dir }}/resources/adguard/extra.env.j2"
adguard_extra_envfile:
# ----------
# The following settings configure Adguard to run as a part of tailnet. This
# allows to configure adguard for a tailnet DNS server under
# https://tailscale.com/admin/dns -> Nameservers
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
#
# NOTE 1: Local network is not accessible within tailnet, which means that if i
# create an adguard DNS rewrite for 'my-site.local' to '192.168.0.20', the site
# will not be reachable in the tailnet. SOLUTION: deploy another tailscale
# service with "advertised routes" (https://tailscale.com/kb/1019/subnets)
adguard_tailscaled: false
adguard_tailscale_version:    # Leave blank to leverage bookshelf configuration
adguard_tailscale_ts_hostname: "{{ adguard_service_name }}-tailscale"
adguard_tailscale_auth_key:

# ==========================
#       AUDIOBOOKSHELF
# ==========================
audiobookshelf_managed: true
audiobookshelf_enabled: true
audiobookshelf_version:       # Leave blank to leverage bookshelf configuration
audiobookshelf_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
audiobookshelf_owner: "{{ ansible_user_id }}"
audiobookshelf_conf_dir: ~/conf/audiobookshelf
audiobookshelf_data_dir: ~/Audiobookshelf        # (REQUIRED)
audiobookshelf_service_name: audiobookshelf
audiobookshelf_hostname: "{{ audiobookshelf_service_name }}"
audiobookshelf_vhost: "{{ audiobookshelf_service_name }}.domain.local"
audiobookshelf_web_ui_port:    # Leave blank to leverage bookshelf configuration
audiobookshelf_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file, will affect both audiobookshelf
# and tailscale. Ex.: "{{ playbook_dir }}/resources/audiobookshelf/extra.env.j2"
audiobookshelf_extra_envfile:
# ----------
# The following settings configure Audiobookshelf to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
audiobookshelf_tailscaled: false
audiobookshelf_tailscale_version:   # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
audiobookshelf_tailscale_ts_hostname: "{{ audiobookshelf_service_name }}-tailscale"
audiobookshelf_tailscale_auth_key:

# =======================
#       CODE-SERVER
# =======================
code_server_managed: "{{ factum_os_family not in ['alpine'] }}"
code_server_version:      # Leave blank to leverage bookshelf configuration
code_server_owner: "{{ ansible_user_id }}"
code_server_web_ui_port:  # Leave blank to leverage bookshelf configuration
code_server_pass: changeme
code_server_extensions:   # <- List of extension
  # - editorconfig.editorconfig
  # - mads-hartmann.bash-ide-vscode
  # - timonwong.shellcheck
  # - foxundermoon.shell-format
  # - redhat.ansible
# If defined, takes precedence over 'code_server_pass'.
# Generate with (replace PASS with your value):
#   printf -- '%s' 'PASS' | docker container run -i --rm alpine /bin/sh -c 'apk add --update --no-cache argon2 openssl && argon2 "$(openssl rand -base64 8)" -e'
# Changeme123
code_server_hashed_pass: "$argon2i$v=19$m=4096,t=3,p=1$WDlYSWhzL2RTUWc9$+KWVsLyUUfaCb4AcvkoCK52uo3z608fQ/ED1mvsCysE"
# Custom config file template. See the default one in templates directory
# code_server_custom_conf: "{{ playbook_dir }}/resources/code-server.config.yaml.j2"
code_server_custom_conf:
# Custom user config file template, applied only on the service initialization.
# See the default one in templates/
# code_server_user_conf: "{{ playbook_dir }}/resources/code-server.user-conf.json.j2"
code_server_user_conf: sane.json.j2

# ============================
#       DOCKER-LOGROTATE
# ============================
docker_logrotate_managed: "{{ docker_managed | default(false) }}"
docker_logrotate_keep_count: 7

# =======================
#       FILEBROWSER
# =======================
# Web File Browser
#   https://filebrowser.org/
# -----
# Versions:
# * https://hub.docker.com/r/filebrowser/filebrowser/tags?name=-s6
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
filebrowser_managed: true
filebrowser_enabled: true
filebrowser_version:        # Leave blank to leverage bookshelf configuration
filebrowser_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
filebrowser_owner: "{{ ansible_user_id }}"
filebrowser_conf_dir: ~/conf/filebrowser
filebrowser_data_dir: ~/Filebrowser      # (REQUIRED)
filebrowser_service_name: filebrowser
filebrowser_hostname: "{{ filebrowser_service_name }}"
filebrowser_vhost: "{{ filebrowser_service_name }}.domain.local"
filebrowser_web_ui_port:    # Leave blank to leverage bookshelf configuration
filebrowser_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file, will affect both filebrowser
# and tailscale. Ex.: "{{ playbook_dir }}/resources/filebrowser/extra.env.j2"
filebrowser_extra_envfile:
# ----------
# The following settings configure FileBrowser to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
filebrowser_tailscaled: false
filebrowser_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
filebrowser_tailscale_ts_hostname: "{{ filebrowser_service_name }}-tailscale"
filebrowser_tailscale_auth_key:

# ==================
#       GOTIFY
# ==================
# Versions:
# * https://hub.docker.com/r/gotify/server/tags
# -----
gotify_managed: true
gotify_enabled: true
gotify_version:       # Leave blank to leverage bookshelf configuration
gotify_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
gotify_owner: "{{ ansible_user_id }}"
gotify_conf_dir: ~/conf/gotify
gotify_service_name: gotify
gotify_hostname: "{{ gotify_service_name }}"
gotify_vhost: "{{ gotify_service_name }}.domain.local"
gotify_web_ui_port:   # Leave blank to leverage bookshelf configuration
gotify_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# ====================
#       HEIMDALL
# ====================
# Versions:
# * https://hub.docker.com/r/linuxserver/heimdall/tags
# -----
heimdall_managed: true
heimdall_enabled: true
heimdall_version:       # Leave blank to leverage bookshelf configuration
heimdall_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
heimdall_owner: "{{ ansible_user_id }}"
heimdall_conf_dir: ~/conf/heimdall
heimdall_service_name: heimdall
heimdall_hostname: "{{ heimdall_service_name }}"
heimdall_vhost: "{{ heimdall_service_name }}.domain.local"
heimdall_web_ui_port:   # Leave blank to leverage bookshelf configuration
heimdall_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# ====================
#       JELLYFIN
# ====================
# Versions:
# * https://hub.docker.com/r/linuxserver/jellyfin/tags
# -----
jellyfin_managed: true
jellyfin_enabled: true
jellyfin_version:       # Leave blank to leverage bookshelf configuration
jellyfin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
jellyfin_owner: "{{ ansible_user_id }}"
jellyfin_conf_dir: ~/conf/jellyfin
jellyfin_data_dir: ~/media     # Directory with media files. Required
jellyfin_service_name: jellyfin
jellyfin_hostname: "{{ jellyfin_service_name }}"
jellyfin_vhost: "{{ jellyfin_service_name }}.domain.local"
jellyfin_web_ui_port:   # Leave blank to leverage bookshelf configuration
jellyfin_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/jellyfin/extra.env.j2"
jellyfin_extra_envfile:
# ----------
# The following settings configure Jellyfin to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jellyfin_tailscaled: false
jellyfin_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
jellyfin_tailscale_ts_hostname: "{{ jellyfin_service_name }}-tailscale"
jellyfin_tailscale_auth_key:

# =========================
#       JOPLIN-SERVER
# =========================
joplin_server_managed: true
joplin_server_enabled: true
joplin_server_version:              # Leave blank to leverage bookshelf configuration
joplin_server_postgres_version:     # Leave blank to leverage bookshelf configuration
joplin_server_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
joplin_server_owner: "{{ ansible_user_id }}"
joplin_server_conf_dir: ~/conf/joplin-server
joplin_server_service_name: joplin-server
joplin_server_hostname: "{{ joplin_server_service_name }}"
joplin_server_vhost: "{{ joplin_server_service_name }}.domain.local"
joplin_server_web_ui_port:          # Leave blank to leverage bookshelf configuration
# Joplin base url. Ex.:
#   https://{{ joplin_server_vhost }}
#   http://{{ ansible_default_ipv4.address }}:{{ joplin_server_web_ui_port | default(bookshelf_const_port.joplin_server.web_ui, true) }}
joplin_server_base_url: http://{{ ansible_default_ipv4.address }}:{{ joplin_server_web_ui_port | default(bookshelf_const_port.joplin_server.web_ui, true) }}
joplin_server_db_pass: changeme
joplin_server_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# =====================
#       MARIADB11
# =====================
# Versions (use 11.* LTS):
# * https://hub.docker.com/_/mariadb
# Exposes:
# * mariadb11_net - mariadb network name
# -----
mariadb11_managed: true
mariadb11_enabled: true
mariadb11_version:      # Leave blank to leverage bookshelf configuration
mariadb11_compose_dir:  # Unless you know ..., leave blank to leverage bookshelf configuration
mariadb11_owner: "{{ ansible_user_id }}"
mariadb11_conf_dir: ~/conf/mariadb11
mariadb11_service_name: mariadb11
mariadb11_hostname: "{{ mariadb11_service_name }}"
mariadb11_tcp_port:   # Leave blank to leverage bookshelf configuration
mariadb11_root_pass: changeme
# mariadb11_users:
#   - name: test-user   # <- Required
#     pass: changeme    # <- Required
mariadb11_users:

# ==================
#       METUBE
# ==================
# Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)
#   https://github.com/alexta69/metube
# -----
# Versions:
# * https://github.com/alexta69/metube/pkgs/container/metube
# -----
metube_managed: true
metube_enabled: true
metube_version:       # Leave blank to leverage bookshelf configuration
metube_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
metube_owner: "{{ ansible_user_id }}"
metube_conf_dir: ~/conf/metube
metube_data_dir: ~/Metube     # (REQUIRED) Directory with media files
metube_service_name: metube
metube_hostname: "{{ metube_service_name }}"
metube_vhost: "{{ metube_service_name }}.domain.local"
metube_web_ui_port:   # Leave blank to leverage bookshelf configuration
metube_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# https://github.com/alexta69/metube?tab=readme-ov-file#configuration-via-environment-variables
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/metube/extra.env.j2"
metube_extra_envfile:

# =======================
#   NGINX-PROXY-MANAGER
# =======================
# Versions:
# * https://hub.docker.com/r/jc21/nginx-proxy-manager/tags
# -----
# # Optional path to extra configuration .env.j2 file
# nginx_proxy_manager_extra_envfile: "{{ playbook_dir }}/resources/npm/extra.env.j2"
nginx_proxy_manager_managed: true
nginx_proxy_manager_enabled: true
nginx_proxy_manager_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_manager_owner: "{{ ansible_user_id }}"
nginx_proxy_manager_conf_dir: ~/conf/nginx-proxy-manager
nginx_proxy_manager_service_name: nginx-proxy-manager
nginx_proxy_manager_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_vhost: "{{ nginx_proxy_manager_service_name }}.domain.local"
nginx_proxy_manager_extra_envfile:

# ===============
#   NGINX-PROXY
# ===============
# Versions:
# * https://hub.docker.com/r/jwilder/nginx-proxy/tags?name=alpine
# Exposes:
# * nxinx_proxy_net - proxy network name
# -----
nginx_proxy_managed: true
nginx_proxy_enabled: true
nginx_proxy_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_owner: "{{ ansible_user_id }}"
nginx_proxy_service_name: nginx-proxy
nginx_proxy_hostname: "{{ nginx_proxy_service_name }}"
nginx_proxy_http_port: "{{ service_port.nginx_proxy_http }}"     # Ex.: '80', no HTTP when empty
nginx_proxy_https_port:     # Ex.: '443', no HTTPS when empty
nginx_proxy_certs_dir:      # Path to certs on the managed machine. Works paired with HTTPS

# ====================
#       OLIVETIN
# ====================
olivetin_managed: true
olivetin_enabled: true
olivetin_version:       # Leave blank to leverage bookshelf configuration
olivetin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
olivetin_owner: "{{ ansible_user_id }}"
olivetin_conf_dir: ~/conf/olivetin
olivetin_service_name: olivetin
olivetin_hostname: "{{ olivetin_service_name }}"
olivetin_vhost: "{{ olivetin_service_name }}.domain.local"
olivetin_web_ui_port:   # Leave blank to leverage bookshelf configuration
olivetin_scripts:
  - sample.sh.j2
olivetin_volumes:
  - '~:/app/home:ro'
olivetin_config:

# ===========================
#       PORTAINER-AGENT
# ===========================
# Versions:
# * https://hub.docker.com/r/portainer/agent/tags?name=alpine
# -----
portainer_agent_managed: true
portainer_agent_enabled: true
portainer_agent_version:        # Leave blank to leverage bookshelf configuration
portainer_agent_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_agent_owner: "{{ ansible_user_id }}"
portainer_agent_service_name: portainer-agent
portainer_agent_hostname: "{{ portainer_service_name }}"
portainer_agent_host_management: true
portainer_agent_tcp_port: 9002  # Leave blank to leverage bookshelf configuration

# =====================
#       PORTAINER
# =====================
# Versions:
# * https://hub.docker.com/r/portainer/portainer-ce/tags?name=alpine
# -----
portainer_managed: true
portainer_enabled: true
portainer_version:        # Leave blank to leverage bookshelf configuration
portainer_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_owner: "{{ ansible_user_id }}"
portainer_conf_dir: ~/conf/portainer
portainer_service_name: portainer
portainer_hostname: "{{ portainer_service_name }}"
portainer_vhost: "{{ portainer_service_name }}.domain.local"
portainer_web_ui_port:    # Leave blank to leverage bookshelf configuration
# Publish ssh tunnel server port
portainer_agent_ready: true

# =======================
#       QBITTORRENT
# =======================
# Free and reliable P2P Bittorrent client
#   https://www.qbittorrent.org/
#   https://hub.docker.com/r/linuxserver/qbittorrent
# -----
# Versions:
# * https://hub.docker.com/r/linuxserver/qbittorrent/tags?name=-ls
# * https://github.com/VueTorrent/VueTorrent/pkgs/container/vuetorrent-lsio-mod/versions
# -----
# NOTE 1: Temporary password for the admin user will be printed to the container
# log on startup
#
# NOTE 2: Switch to alternative UI
#   Settings > WebUI > check Use alternative WebUI, Files location: /vuetorrent
qbittorrent_managed: true
qbittorrent_enabled: true
qbittorrent_version:              # Leave blank to leverage bookshelf configuration
qbittorrent_vuetorrent_version:   # Leave blank to leverage bookshelf configuration
qbittorrent_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
qbittorrent_owner: "{{ ansible_user_id }}"
qbittorrent_conf_dir: ~/conf/qbittorrent
qbittorrent_data_dir: ~/Torrent
qbittorrent_service_name: qbittorrent
qbittorrent_hostname: "{{ qbittorrent_service_name }}"
qbittorrent_vhost: "{{ qbittorrent_service_name }}.domain.local"
qbittorrent_web_ui_port:          # Leave blank to leverage bookshelf configuration
qbittorrent_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional. Will be available in the container in
#   /scripts/downloaded.sh (0700)
#   /scripts/downloaded.secret.sh (0600)
# Configure in qBittorrent UI -> Settings -> Downloads -> External ... on finished
# For demo scripts see:
#   templates/gotify-push.*
#   templates/discord-push.*
# Ex.: "{{ playbook_dir }}/resources/qbittorrent/downloaded.sh.j2"
qbittorrent_downloaded_script: gotify-push.sh.j2
qbittorrent_push_server: https://gotify.net.kos.giize.com
# Optional, can be used in conjunction with qbittorrent_downloaded_script.
# Will only be used when qbittorrent_downloaded_script is set
# Ex.: "{{ playbook_dir }}/resources/qbittorrent/downloaded.secret.sh.j2"
qbittorrent_downloaded_secret_file: gotify-push.secret.sh.j2
qbittorrent_gotify_token: changeme
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/qbittorrent/extra.env.j2"
qbittorrent_extra_envfile:

# =================
#       SAMBA
# =================
# Versions:
# * https://hub.docker.com/r/crazymax/samba/tags
# -----
samba_managed: true
samba_enabled: true
samba_version:        # Leave blank to leverage bookshelf configuration
samba_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
samba_owner: "{{ ansible_user_id }}"
samba_shares_dir: ~/Samba    # (REQUIRED) Shares root directory
samba_service_name: samba
samba_hostname: "{{ samba_service_name }}"
samba_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/samba/extra.env.j2"
samba_extra_envfile:

# =====================
#       SYNCTHING
# =====================
# Versions:
# * https://hub.docker.com/r/linuxserver/syncthing/tags?name=-ls
# -----
syncthing_managed: true
syncthing_enabled: true
syncthing_version:              # Leave blank to leverage bookshelf configuration
syncthing_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
syncthing_owner: "{{ ansible_user_id }}"
syncthing_conf_dir: ~/conf/syncthing
syncthing_data_dir: ~/Syncthing
syncthing_service_name: syncthing
syncthing_hostname: "{{ syncthing_service_name }}"
syncthing_vhost: "{{ syncthing_service_name }}.domain.local"
syncthing_web_ui_port:          # Leave blank to leverage bookshelf configuration
syncthing_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# ----------
# The following settings configure Syncthing to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
syncthing_tailscaled: false
syncthing_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
syncthing_tailscale_ts_hostname: "{{ syncthing_service_name }}-tailscale"
syncthing_tailscale_auth_key:

# =====================
#       TAILSCALE
# =====================
# Versions:
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
# IMPORTANT 1: Auth key is required for initial container run:
# https://tailscale.com/admin/settings/keys -> 'Generate auth key...'
#   -> generate single time or reusable
#
# IMPORTANT 2: After deployment https://tailscale.com/admin/machines -> 3 dots
# next to the machine -> Disable key expiry (to avoid re-login to tailscale)
#
# IMPORTANT 3: With changed routes or exit node params service requires a valid
# auth key, i.e. if it's one-timer new one must be provided
#
# NOTE 1: The service is useless unless it's configured for exit node or subnet
# router. Both need to be confirmed in the machines admin
tailscale_managed: false
tailscale_enabled: false
tailscale_version:          # Leave blank to leverage bookshelf configuration
tailscale_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
tailscale_owner: "{{ ansible_user_id }}"
tailscale_conf_dir: ~/conf/tailscale
tailscale_service_name: tailscale
tailscale_hostname: "{{ tailscale_service_name }}"
tailscale_auth_key:
# Ex.: '192.168.0.0/24,192.168.2.0/24'
tailscale_routes:
tailscale_exit_node: false  # Advertise exit node
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/tailscale/extra.env.j2"
tailscale_extra_envfile:

# ====================
#       TWINGATE
# ====================
# Versions:
# * https://hub.docker.com/r/twingate/connector/tags
# -----
twingate_managed: false
twingate_enabled: false
twingate_version:         # Leave blank to leverage bookshelf configuration
twingate_compose_dir:     # Unless you know ..., leave blank to leverage bookshelf configuration
twingate_owner: "{{ ansible_user_id }}"
twingate_service_name: twingate
twingate_hostname: "{{ twingate_service_name }}"
twingate_tenant_name:     # Your twingate network name. Required
# https://<tenant>.twingate.com/networks/overview -> 'Deploy ... Connector'
#   -> 'Docker' -> 'Generate Tokens'
twingate_access_token:    # Required
twingate_refresh_token:   # Required
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/twingate/extra.env.j2"
twingate_extra_envfile:

# =======================
#       VAULTWARDEN
# =======================
# The community driven password manager.
#   https://www.vaultwarden.net/
# -----
# Versions:
# * https://hub.docker.com/r/vaultwarden/server/tags?name=alpine
# Options description:
# * https://github.com/dani-garcia/vaultwarden/blob/main/.env.template
# -----
vaultwarden_managed: true
vaultwarden_enabled: true
vaultwarden_version:        # Leave blank to leverage bookshelf configuration
vaultwarden_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
vaultwarden_owner: "{{ ansible_user_id }}"
vaultwarden_conf_dir: ~/conf/vaultwarden
vaultwarden_service_name: vaultwarden
vaultwarden_hostname: "{{ vaultwarden_service_name }}"
vaultwarden_vhost: "{{ vaultwarden_service_name }}.domain.local"
# Required with https via reverse proxy, vaultwarden needs to know it's https
# to work properly with attachments.
# https://github.com/dani-garcia/vaultwarden/wiki/Using-Docker-Compose
# Ex.: https://{{ vaultwarden_vhost }}
vaultwarden_vdomain: https://{{ vaultwarden_vhost }}
# Access token for http(s)://VAULTWARDEN_HOST/admin
# Generate with:
#   docker container run -it --rm vaultwarden/server:latest /vaultwarden hash
vaultwarden_admin_token_hash:
vaultwarden_web_ui_port:    # Leave blank to leverage bookshelf configuration
vaultwarden_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/vaultwarden/extra.env.j2"
vaultwarden_extra_envfile:

# ===================
#       WG-EASY
# ===================
# Versions:
# * https://github.com/wg-easy/wg-easy/pkgs/container/wg-easy/versions?filters%5Bversion_type%5D=tagged
# Options description:
# * https://github.com/wg-easy/wg-easy/tree/production?tab=readme-ov-file#options
# * https://github.com/wg-easy/wg-easy/blob/production/How_to_generate_an_bcrypt_hash.md
# -----
wg_easy_managed: true
wg_easy_enabled: true
wg_easy_version:        # Leave blank to leverage bookshelf configuration
wg_easy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
wg_easy_owner: "{{ ansible_user_id }}"
wg_easy_conf_dir: ~/conf/wg-easy
wg_easy_service_name: wg-easy
wg_easy_hostname: "{{ wg_easy_service_name }}"
wg_easy_vhost: "{{ wg_easy_service_name }}.domain.local"
# Defaults to defined in WG-Easy. Ex.: '8.4.4.8,10.0.0.1'
wg_easy_dns: 8.4.4.8
# Defaults to no-pass login. See options description for how to generate
wg_easy_pass_hash:
# Required. Ex.: 'my-vpn-server.public-domain.com' or '112.2.84.234'
wg_easy_host: foo.bar   # Required. Ex.: 'my.domain.com' or '8.8.8.8'
wg_easy_web_ui_port:    # Leave blank to leverage bookshelf configuration
wg_easy_udp_port: 51820
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/wg-easy/extra.env.j2"
wg_easy_extra_envfile:



###################
###   DESKTOP   ###
###################

# ============
#   AUDACITY
# ============
audacity_managed: false

# =================
#       BRAVE
# =================
brave_managed: false

# ==================
#       CHROME
# ==================
# Official web browser from Google, built to be fast, secure, and customizable.
#   https://www.google.com/chrome/
# -----
chrome_managed: false

# =================
#       COPYQ
# =================
# copyq_users:  # <- Existing users to have copyq autostarted
#   - "{{ ansible_user | default(ansible_user_id) }}"
copyq_managed: false
# 'pm' or 'flatpak'. With non-ubuntu always forced to 'flatpak'
copyq_install_method: pm
copyq_users:

# ===================
#       DBEAVER
# ===================
dbeaver_managed: false
# 'snap' or 'flatpak'. With non-ubuntu always forced to 'flatpak'
dbeaver_install_method: flatpak

# ===========
#   FLATPAK
# ===========
# flatpak_repos:  # <- These are all supported repos
#   - flathub
flatpak_managed: false
flatpak_repos:
  - flathub

# ================
#       GIMP
# ================
gimp_managed: false

# ======================
#       INPUT-LEAP
# ======================
# Open-source KVM software (Barrier fork)
#   https://github.com/input-leap/input-leap
# -----
# input_leap_users:  # <- Existing users to have InputLeap autostarted
#   - "{{ ansible_user_id }}"
input_leap_managed: false
input_leap_users:

# =========================
#       INTELLIJ-IDEA
# =========================
# The IDE for Java and Kotlin
#   https://www.jetbrains.com/idea/
# -----
intellij_idea_managed: false
# Ultimate edition, unless community
intellij_idea_ultimate: false

# ==========================
#       JOPLIN-DESKTOP
# ==========================
joplin_desktop_managed: false
# Supported values in prefered order (due newer versions in snap):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
joplin_desktop_install_method: snap

# ====================
#       KDENLIVE
# ====================
kdenlive_managed: false
# 'snap' or 'flatpak'
kdenlive_install_method: snap

# ======================
#       MKVTOOLNIX
# ======================
mkvtoolnix_managed: false

# ====================
#       NXPLAYER
# ====================
nxplayer_managed: false

# ======================
#       OBS-STUDIO
# ======================
obs_studio_managed: false

# =================
#       OPERA
# =================
opera_managed: false

# ============
#   PHPSTORM
# ============
phpstorm_managed: false

# =================
#       PINTA
# =================
pinta_managed: false
# 'snap' or 'flatpak'
pinta_install_method: flatpak

# ===================
#       POSTMAN
# ===================
postman_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
postman_install_method: flatpak

# ===========
#   SUBLIME
# ===========
sublime_managed: false
sublime_merge: false
# Supported values in prefered order (due to available CLI interface):
# * pm    # <- Forced for non-Ubuntu-like
# * snap
sublime_install_method: pm

# =========
#   TEAMS
# =========
# teams_users:  # <- Existing users to have teams autostarted
#   - "{{ ansible_user | default(ansible_user_id) }}"
teams_managed: false
teams_users:

# ======================
#       TERMINATOR
# ======================
terminator_managed: false

# =========
#   VIBER
# =========
# Currently only Debian-based platforms supported
# -----
# viber_users:  # <- Existing users to have viber autostarted
#   - "{{ ansible_user | default(ansible_user_id) }}"
viber_managed: false
viber_users:

# =======
#   VLC
# =======
vlc_managed: false

# ==========
#   VSCODE
# ==========
vscode_managed: false
# Supported values in prefered order (due to available CLI interface):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
vscode_install_method: snap
vscode_extensions:

# ===================
#       WHATSIE
# ===================
whatsie_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
whatsie_install_method: flatpak
