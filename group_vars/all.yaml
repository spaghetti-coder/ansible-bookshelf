---
#
# THIS IS A DEMO CONFIGURATION MOSTLY FOR TESTING
#

# *_managed vars denote whether ansible takes care of the application or not.
#   `X_managed: false` application will still fire if it is in `Y_managed: true`
#   application dependencies
#
# *_enabled vars denote whether the application is enabled or not. Unlike
#   *_managed it disables the application when `*_enabled: false`

service_port:
  nginx_proxy_http: "{{ (caddy_enabled | default(false)) | ternary('9080', '80') }}"

###
### Server connection configuration
###
# Don't keep passwords in plain text, encrypt them. One option would be:
#   ansible-vault encrypt_string -J PASSWORD_PLACEHOLDER
ansible_user: "{{ lookup('env', 'ANSIBLE_USER') }}"
ansible_password: "{{ lookup('env', 'ANSIBLE_USER_PASS') }}"
ansible_become_password: "{{ lookup('env', 'ANSIBLE_USER_PASS') }}"

#################### {{ ROLES_CONF_TS4LE64m91 }} ####################


################
###   BASE   ###
################

# ===============
#       AGE
# ===============
# VERSIONS: https://github.com/FiloSottile/age/releases
# -----
age_managed: true
age_version:    # Leave blank to leverage bookshelf configuration

# =============================
#       ALERT-MAX-TRAFFIC
# =============================
alert_max_traffic_managed: false
# notify-sh configuration
alert_max_traffic_provider:    # (REQUIRED) See supported in notify-sh role
alert_max_traffic_token:       # (REQUIRED)
alert_max_traffic_server:      # (REQUIRED for gotify)
alert_max_traffic_chat_id:     # (REQUIRED for telegram)
# Traffic alert settings
alert_max_traffic_interface:   # (REQUIRED) Network interface
# Max total traffic in MiB
alert_max_traffic_max_day:     # (OPTIONAL)
alert_max_traffic_max_month:   # (OPTIONAL)

# =====================
#       ALERT-SSH
# =====================
# SSH login notificator.
# Inspired by https://www.youtube.com/watch?v=VzH4NfJMiQc
# -----
alert_ssh_managed: false   #  "{{ factum_os_family not in ['alpine'] }}"   # <- Not supported by Alpine
alert_ssh_provider: gotify              # (REQUIRED) See supported in notify-sh role
alert_ssh_token: 123                    # (REQUIRED)
alert_ssh_server: https://gotify.home   # (REQUIRED for gotify)
alert_ssh_chat_id:                      # (REQUIRED for telegram)

# =====================
#       BASH-SANE
# =====================
bash_sane_managed: true
bash_sane_users:    # <- Existing users to use bash set settings
  - "{{ ansible_user_id }}"

# ================
#       BASH
# ================
bash_managed: true
bash_completion: true
bash_users:   # <- Existing users to use bash by default
  - "{{ ansible_user_id }}"

# ========================
#       DOCKER-CLEAN
# ========================
docker_clean_managed: "{{ docker_managed | default(false) }}"   # <- When docker installed
docker_clean_logrotate_keep_count: 7
docker_clean_prune_images: true     # <- Weekly prune unused images

# ==========
#   DOCKER
# ==========
docker_managed: true
docker_users:   # <- Users to be added to docker group
  - "{{ ansible_user_id }}"

# =========================
#       DYDNS-UPDATER
# =========================
dydns_updater_managed: true
dydns_updater_prereqs: true
dydns_updater_providers:    # <- Supported list
  - duckdns
  # - dynu
dydns_updater_secret: |     # <- Will go to secret file
  DUCKDNS_TOKEN=123
#   DYNU_TOKEN=456
#   # ... Same for other providers ...
dydns_updater_config: |     # <- Will go to config file
  DUCKDNS_DOMAINS=(                    `# <- (REQUIRED)`
    site1.duckdns.org
    site2.duckdns.org
  )
  DUCKDNS_SCHEDULE='0 22 * * *'         # <- (OPTIONAL)
  DUCKDNS_ERR_TITLE='DuckDNS failure'   # <- (OPTIONAL)
#   # ... Same for other providers ...
# Extra providers to go to extra file
dydns_updater_extra: |      # "{{ lookup('file', playbook_dir + '/resources/dydns.extra.sh') }}"
  #!/usr/bin/env bash
  FOO=bar

# =========
#   ENVAR
# =========
envar_managed: true
envar_users:  # <- Existing users to have access to envar
  - "{{ ansible_user_id }}"
  - root

# ===============
#       FZF
# ===============
# VERSIONS: https://github.com/junegunn/fzf/releases
# -----
fzf_managed: true
fzf_version:    # Leave blank to leverage bookshelf configuration
fzf_users:      # <- Existing users to have fzf integrated
  - "{{ ansible_user | default(ansible_user_id) }}"
  - root

# =======
#   GIT
# =======
git_managed: true
git_extraconf:
  - owner: "{{ ansible_user_id }}"
    source: gitconfig.extra.ini.j2
    # Optional variables, can be addressed in the template via 'conf' map in case of template source
    user_name: Foo
    user_email: foo@bar.baz
  # - owner: another-user
  #   source: "{{ playbook_dir }}/resources/git/extra.ini"  # <- Handled as file due to not '*.j2'
  #   # No variables, as it's not a template
  # # ...

# =====================
#       NOTIFY-SH
# =====================
notify_sh_managed: true
notify_sh_prereqs: true  # Install notify.sh prepreqs

# ===========
#   PS1-GIT
# ===========
ps1_git_managed: true
ps1_git_users:  # <- Existing users to have this PS1
  - "{{ ansible_user_id }}"
  - root

# =========
#   SNAPD
# =========
snapd_managed: "{{ factum_os_like in ['ubuntu'] }}"  # <- Only supported by Ubuntu like

# ============================
#       TAILSCALE-CLIENT
# ============================
tailscale_client_managed: true

# ==============
#      TMUX
# ==============
tmux_managed: true
tmux_sane_users:   # <- Existing users to source my preferred configuration
  - "{{ ansible_user_id }}"

# ===============
#      TMUXP
# ===============
tmuxp_managed: "{{ factum_os_family not in ['alpine'] }}"   # <- Not supported by Alpine
tmuxp_conf:
  - owner: "{{ ansible_user_id }}"
    conf:
      - source: sensors.yaml.j2
        name: sensors
        # Optional variables, can be addressed in the template via 'conf' map in case of template source
        session_name: Sensors
        window_name: sensors
      - source: triple.yaml.j2
        name: triple
        pane3_start_dir: ~/tmp
  #     - source: "{{ playbook_dir }}/resources/tmuxp/demo1.yaml"   # <- Handled as file due to not '*.j2'
  #       name: demo1
  #       # No variables, as it's not a template
  # - owner: # ...
  #   # ...

# ====================
#       TOOLBELT
# ====================
# Contains very basic and often needed tools that for most often usage don't
# require any additional configuration. Some of the tools are categorized and
# the categories can be used to install the whole set of underlying tools.
# -----
# * editors:      nano, neovim, vim
# * downloaders:  curl, wget
# * viewers:      bat, glow
# * htop, jq, sensors, skate, speedtest, tar, wishlist
# -----
toolbelt_managed: true
toolbelt_pick:      # <- Install only a fraction. Leave empty to install all
  # - nano            # <- nano will be installed
  # - downloaders     # <- All downloaders will be installed
  # - tar             # <- tar will be installed
toolbelt_exclude:   # <- Opposed to toolbelt_pick, higher priority than toolbelt_pick
  # - wget            # <- In conjunction with toolbelt_pick above, only curl will be installed
  # - editors         # <- Dispite toolbelt_pick above, nano will not be installed

# ========================
#       VIRT-MANAGER
# ========================
virt_manager_managed: "{{ factum_os_family in ['debian'] }}"  # <- Only Debian family is supported
virt_manager_users:   # <- Existing users to be added to libvirt group
  - "{{ ansible_user_id }}"

# ==================
#       VNSTAT
# ==================
# Console-based network traffic monitor that keeps a log of daily network
# traffic for the selected interface(s)
vnstat_managed: true



###################   # All docker services are only tested in alpine
###   SERVICE   ###
###################

# ===================
#       ADGUARD
# ===================
# VERSIONS:
#   * https://hub.docker.com/r/adguard/adguardhome/tags
#   * => See tailscale role versions
# -----
adguard_managed: "{{ factum_os_family in ['alpine'] }}"   # "{{ factum_os_like not in ['ubuntu'] }}"   # <- All hosts but Ubuntu
adguard_enabled: true
adguard_version:        # Leave blank to leverage bookshelf configuration
adguard_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
adguard_owner: "{{ ansible_user_id }}"
adguard_conf_dir: ~/conf/adguard
adguard_service_name: adguard
adguard_hostname: "{{ adguard_service_name }}"
adguard_vhost: "{{ adguard_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
adguard_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
adguard_init_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
adguard_dns_ports_exposed: true
adguard_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both adguard and tailscale.
adguard_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/adguard.env.j2') }}"
  # References:
  #   * Adguard is not really configurable via env vars
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
# ----------
# The following settings configure Adguard to run as a part of tailnet. This
# allows to configure adguard for a tailnet DNS server under
# https://tailscale.com/admin/dns -> Nameservers
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
#
# NOTE 1: Local network is not accessible within tailnet, which means that if i
# create an adguard DNS rewrite for 'my-site.local' to '192.168.0.20', the site
# will not be reachable in the tailnet. SOLUTION: deploy another tailscale
# service with "advertised routes" (https://tailscale.com/kb/1019/subnets)
adguard_tailscaled: false
adguard_ts_version:    # Leave blank to leverage bookshelf configuration
adguard_ts_service_name: "{{ adguard_service_name }}-ts"
adguard_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
adguard_ts_hostname: "{{ adguard_service_name }}"
adguard_ts_auth_key:

# ==========================
#       AUDIOBOOKSHELF
# ==========================
# VERSIONS:
#   * https://hub.docker.com/r/advplyr/audiobookshelf/tags
#   * => See tailscale role versions
# -----
audiobookshelf_managed: "{{ factum_os_family in ['alpine'] }}"
audiobookshelf_enabled: true
audiobookshelf_version:       # Leave blank to leverage bookshelf configuration
audiobookshelf_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
audiobookshelf_owner: bug2    # <- Who runs the container
audiobookshelf_user: "{{ ansible_user_id }}"    # <- The in-container user and data filesystem owner
audiobookshelf_conf_dir: ~/conf/audiobookshelf
audiobookshelf_data_volumes:  # <- Volumes for data
  - ~/data/Audiobooks:/data   # (REQUIRED) At least one volume required
  - ~/data/Children/Audiobooks:/extra/Children
  - ~/data/Parents/Audiobooks:/extra/Parents
audiobookshelf_service_name: audiobookshelf
audiobookshelf_hostname: "{{ audiobookshelf_service_name }}"
audiobookshelf_vhost: "{{ audiobookshelf_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
audiobookshelf_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
audiobookshelf_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both audiobookshelf and tailscale.
audiobookshelf_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/audiobookshelf.env.j2') }}"
  # References:
  #   * https://www.audiobookshelf.org/docs#env-configuration
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # PORT=5555
# ----------
# The following settings configure Audiobookshelf to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
audiobookshelf_tailscaled: false
audiobookshelf_ts_version:   # Leave blank to leverage bookshelf configuration
audiobookshelf_ts_service_name: "{{ audiobookshelf_service_name }}-ts"
audiobookshelf_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
audiobookshelf_ts_hostname: "{{ audiobookshelf_service_name }}"
audiobookshelf_ts_auth_key:

# ==================
#       BLINKO
# ==================
# An open-source, self-hosted personal AI note tool prioritizing privacy,
# built using TypeScript.
#   https://blinko.space/en
# -----
# VERSIONS:
#   * https://hub.docker.com/r/blinkospace/blinko/tags
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
blinko_managed: "{{ factum_os_family in ['alpine'] }}"
blinko_enabled: true
blinko_version:             # Leave blank to leverage bookshelf configuration
blinko_postgres_version:    # Leave blank to leverage bookshelf configuration
blinko_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
blinko_owner: "{{ ansible_user_id }}"
blinko_conf_dir: ~/conf/blinko
blinko_service_name: blinko
blinko_hostname: "{{ blinko_service_name }}"
blinko_vhost: "{{ blinko_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
blinko_web_ui_port:         # Leave blank for bookshelf default, or '-1' to not expose the port
blinko_db_service_name: "{{ blinko_service_name }}-db"
blinko_db_pass: foobarbaz            # (REQUIRED) Something hard and random
blinko_nextauth_secret: foobarbaz    # (REQUIRED)
blinko_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Blinko and Tailscale.
blinko_extra_env: |         # "{{ lookup('template', playbook_dir + '/resources/blinko.env.j2') }}"
  # References:
  #   * https://docs.blinko.space/en/install#docker-installation
  #   * https://hub.docker.com/_/postgres#environment-variables
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # NEXTAUTH_URL=...
# ----------
# The following settings configure Blinko to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
blinko_tailscaled: false
blinko_ts_version:          # Leave blank to leverage bookshelf configuration
blinko_ts_service_name: "{{ blinko_service_name }}-ts"
blinko_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
blinko_ts_hostname: "{{ blinko_service_name }}"
blinko_ts_auth_key:

# =================
#       CADDY
# =================
# Fast and extensible multi-platform HTTP/1-2-3 web server with automatic HTTPS
#   https://caddyserver.com/
# -----
# VERSIONS:
#   * https://hub.docker.com/_/caddy/tags
# -----
caddy_managed: "{{ factum_os_family in ['alpine'] }}"
caddy_enabled: true
caddy_version:          # Leave blank to leverage bookshelf configuration
caddy_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
caddy_owner: "{{ ansible_user_id }}"
caddy_conf_dir: ~/conf/caddy
caddy_service_name: caddy
caddy_hostname: "{{ caddy_service_name }}"
caddy_modules:          # https://caddyserver.com/docs/modules/
  - github.com/caddy-dns/desec
  # - github.com/caddy-dns/duckdns
  # - github.com/caddy-dns/dynu
caddy_cert_bundles:
  - text: "{{ lookup('file', 'demo1.bundle.pem.b64') | b64decode }}"
    name: test.local.pem    # <- Will land in /etc/ssl/private/test.local.pem
caddy_caddyfile: "{{ lookup('file', 'demo1.Caddyfile') }}"
caddy_host_net: false
# Port settings only get applied with caddy_host_net == false
caddy_http_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
caddy_https_port:       # Leave blank for bookshelf default, or '-1' to not expose the port
caddy_vhost: "{{ caddy_service_name }}.domain.local"
# Optional extra env vars
caddy_extra_env: |      # "{{ lookup('template', playbook_dir + '/resources/caddy.env.j2') }}"
  # Can be used to pass values to Caddyfile
  # ----------
  # DESEC_BASE=demo.dedyn.io
caddy_secrets_env: |      # "{{ lookup('template', playbook_dir + '/resources/caddy.secret.env.j2') }}"
  # Can be used to pass secret values to Caddyfile
  # ----------
  # DESEC_TOKEN=123qwerty456

# =======================
#       CODE-SERVER
# =======================
# VERSIONS: https://github.com/coder/code-server/releases
# -----
code_server_managed: "{{ factum_os_family not in ['alpine'] }}"
code_server_version:      # Leave blank to leverage bookshelf configuration
code_server_owner: "{{ ansible_user_id }}"
code_server_web_ui_port:  # Leave blank to leverage bookshelf configuration
code_server_extensions:   # <- List of extension
  # - editorconfig.editorconfig
  # - mads-hartmann.bash-ide-vscode
  # - timonwong.shellcheck
  # - foxundermoon.shell-format
  # - redhat.ansible
code_server_pass: changeme
# If defined, takes precedence over 'code_server_pass'.
# Generate with (replace PASS with your value):
#   printf -- '%s' 'PASS' | docker container run -i --rm alpine /bin/sh -c 'apk add --update --no-cache argon2 openssl && argon2 "$(openssl rand -base64 8)" -e'
# Changeme123
code_server_hashed_pass: "$argon2i$v=19$m=4096,t=3,p=1$WDlYSWhzL2RTUWc9$+KWVsLyUUfaCb4AcvkoCK52uo3z608fQ/ED1mvsCysE"
code_server_custom_conf:    # "{{ lookup('template', playbook_dir + '/resources/code-server.config.yaml.j2') }}"
code_server_user_conf:      # "{{ lookup('template', 'sane.json.j2') }}"

# ======================
#       CRONMASTER
# ======================
# Cron management made easy
#   https://github.com/fccview/cronmaster
# -----
# VERSIONS:
#   * https://github.com/fccview/cronmaster/pkgs/container/cronmaster
#   * => See tailscale role versions
# -----
cronmaster_managed: "{{ factum_os_family in ['alpine'] }}"
cronmaster_enabled: true
cronmaster_version:             # Leave blank to leverage bookshelf configuration
cronmaster_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
cronmaster_owner: "{{ ansible_user_id }}"
cronmaster_conf_dir: ~/conf/cronmaster
cronmaster_service_name: cronmaster
cronmaster_hostname: "{{ cronmaster_service_name }}"
cronmaster_vhost: "{{ cronmaster_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
cronmaster_web_ui_port:         # Leave blank for bookshelf default, or '-1' to not expose the port
cronmaster_auth_pass: changeme  # (REQUIRED)
cronmaster_crontab_users:       # List of crontab users (from host)
  - root
  - "{{ ansible_user_id }}"
cronmaster_scripts:
  - text: "{{ lookup('template', 'demo-script.sh.j2') }}"  # <- See this one for demo, uses demo-secret.txt
    filename: demo-script.sh        # Destination filename, should not intersect with existing scripts
cronmaster_snippets:  # <- Guide: https://github.com/fccview/cronmaster/blob/main/snippets/README.md
  - text: "{{ lookup('file', 'demo-snippet.sh') }}"
    filename: demo-snippet.sh
cronmaster_secrets:   # <- Will land in /secrets directory inside the container
  - text: "{{ lookup('file', 'demo-secret.txt') }}"
    filename: demo-secret.txt       # See demo-script.sh.j2 to see how to reference the secret
    owner: "{{ ansible_user_id }}"  # Optional, defaults to root. The file is only RW by the owner
# Optional extra env vars, will affect both CronMaster and Tailscale.
cronmaster_extra_env: |         # "{{ lookup('template', playbook_dir + '/resources/cronmaster.env.j2') }}"
  # References:
  #   * https://github.com/fccview/cronmaster#environment-variables
  #   * https://hub.docker.com/_/postgres#environment-variables
  # ----------
  # NEXT_PUBLIC_CLOCK_UPDATE_INTERVAL=60000
# ----------
# The following settings configure CronMaster to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
cronmaster_tailscaled: false
cronmaster_ts_version:          # Leave blank to leverage bookshelf configuration
cronmaster_ts_service_name: "{{ cronmaster_service_name }}-ts"
cronmaster_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
cronmaster_ts_hostname: "{{ cronmaster_service_name }}"
cronmaster_ts_auth_key:

# =====================
#       DUPLICATI
# =====================
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/duplicati/tags?name=-ls
#   * => See tailscale role versions
# -----
duplicati_managed: "{{ factum_os_family in ['alpine'] }}"
duplicati_enabled: true
duplicati_version:        # Leave blank to leverage bookshelf configuration
duplicati_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
duplicati_owner: "{{ ansible_user_id }}"    # <- Who runs the container
duplicati_conf_dir: ~/conf/duplicati
duplicati_backup_volumes:                   # <- Mounts for backups, at least one entry required
  - ~/backup/Main:/backups                  # <- One of data volumes must be mounted to '/backups'
  - ~/backup/Photos:/extra/backups/Photos
  - ~/backup/Important:/extra/backups/Important
# '~' refers to the owner HOME.
duplicati_data_volumes:                     # <- Mounts for data, at least one entry required
  - volume: ~/data:/source                  # <- One of data volumes must be mounted to '/source'
    owner: bug2                             # <- (OPTIONAL) Host directory owner
  - volume: ~/data1/Photos:/extra/source/Photos    # <- Owner is duplicati_owner
  - ~/data1/Important:/extra/source/Important      # <- Owner is duplicati_owner
duplicati_service_name: duplicati
duplicati_hostname: "{{ duplicati_service_name }}"
duplicati_vhost: "{{ duplicati_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
duplicati_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
duplicati_enc_key: changeme   # (REQUIRED) Min 8 symbols
duplicati_pass:           # (OPTIONAL)
duplicati_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Duplicati and tailscale.
duplicati_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/duplicati.env.j2') }}"
  # References:
  #   * https://prev-docs.duplicati.com/en/latest/07-other-command-line-utilities/
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # CLI_ARGS=--server-datafolder=/root/.config/Duplicati
# ----------
# The following settings configure Duplicati to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
duplicati_tailscaled: false
duplicati_ts_version:           # Leave blank to leverage bookshelf configuration
duplicati_ts_service_name: "{{ duplicati_service_name }}-ts"
duplicati_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
duplicati_ts_hostname: "{{ duplicati_service_name }}"
duplicati_ts_auth_key:

# =======================
#       FILEBROWSER
# =======================
# VERSIONS:
#   * https://hub.docker.com/r/filebrowser/filebrowser/tags?name=-s6
#   * => See tailscale role versions
# -----
filebrowser_managed: "{{ factum_os_family in ['alpine'] }}"
filebrowser_enabled: true
filebrowser_version:        # Leave blank to leverage bookshelf configuration
filebrowser_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
filebrowser_owner: bug2    # <- Who runs the container
filebrowser_user: "{{ ansible_user_id }}"   # <- The in-container user and data filesystem owner
filebrowser_conf_dir: ~/conf/filebrowser
filebrowser_data_dir: ~/data
filebrowser_service_name: filebrowser
filebrowser_hostname: "{{ filebrowser_service_name }}"
filebrowser_vhost: "{{ filebrowser_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
filebrowser_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
filebrowser_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
filebrowser_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/filebrowser.env.j2') }}"
  # References:
  #   * FileBrowser is not really configurable via env vars
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
# ----------
# The following settings configure FileBrowser to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
filebrowser_tailscaled: false
filebrowser_ts_version:           # Leave blank to leverage bookshelf configuration
filebrowser_ts_service_name: "{{ filebrowser_service_name }}-ts"
filebrowser_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
filebrowser_ts_hostname: "{{ filebrowser_service_name }}"
filebrowser_ts_auth_key:

# =================
#       GATUS
# =================
# Automated developer-oriented status page.
#   https://github.com/TwiN/gatus
# -----
# VERSIONS:
#   * https://github.com/twin/gatus/pkgs/container/gatus
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
gatus_managed: "{{ factum_os_family in ['alpine'] }}"
gatus_enabled: true
gatus_version:            # Leave blank to leverage bookshelf configuration
gatus_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
gatus_owner: "{{ ansible_user_id }}"
gatus_conf_dir: ~/conf/gatus
gatus_service_name: gatus
gatus_hostname: "{{ gatus_service_name }}"
gatus_vhost: "{{ gatus_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gatus_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
gatus_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
gatus_config: "{{ lookup('file', 'demo.yaml') }}"   # <- (REQUIRED) Replace with yours
# Optional extra env vars, will affect both Gatus and Tailscale.
gatus_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/gatus.env.j2') }}"
  # Gatus allows $ENV_VAR in the config files
  # ----------
  # SOME_VAR=some-val
gatus_secrets_env: |      # "{{ lookup('template', playbook_dir + '/resources/gatus.secret.env.j2') }}"
  # Same as extra env, but for secrets
  # ----------
  # SOME_SECRET_VAR=some-secret
# ----------
# The following settings configure Gatus to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gatus_tailscaled: false
gatus_ts_version:           # Leave blank to leverage bookshelf configuration
gatus_ts_service_name: "{{ gatus_service_name }}-ts"
gatus_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gatus_ts_hostname: "{{ gatus_service_name }}"
gatus_ts_auth_key:

# ==================
#       GOKAPI
# ==================
# Lightweight selfhosted Firefox Send alternative without public upload. AWS S3 supported.
#   https://github.com/Forceu/gokapi
# -----
# VERSIONS:
#   * https://hub.docker.com/r/f0rc3/gokapi/tags
#   * => See tailscale role versions
# -----
gokapi_managed: "{{ factum_os_family in ['alpine'] }}"
gokapi_enabled: true
gokapi_version:            # Leave blank to leverage bookshelf configuration
gokapi_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
gokapi_owner: "{{ ansible_user_id }}"
gokapi_conf_dir: ~/conf/gokapi
gokapi_service_name: gokapi
gokapi_hostname: "{{ gokapi_service_name }}"
gokapi_vhost: "{{ gokapi_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gokapi_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
gokapi_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Gokapi and Tailscale.
gokapi_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/gokapi.env.j2') }}"
  # References:
  #   * https://gokapi.readthedocs.io/en/latest/advanced.html#environment-variables
  # NOTES:
  #   * Don't use the following variables:
  #     * DOCKER_NONROOT
  #     * GOKAPI_PORT
  # ----------
  # GOKAPI_CHUNK_SIZE_MB=80
# ----------
# The following settings configure Gokapi to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gokapi_tailscaled: false
gokapi_ts_version:          # Leave blank to leverage bookshelf configuration
gokapi_ts_service_name: "{{ gokapi_service_name }}-ts"
gokapi_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gokapi_ts_hostname: "{{ gokapi_service_name }}"
gokapi_ts_auth_key:

# ==================
#       GOTIFY
# ==================
# VERSIONS: https://hub.docker.com/r/gotify/server/tags
# -----
gotify_managed: false     # <- LOW INTEREST, no testing
gotify_enabled: true
gotify_version:       # Leave blank to leverage bookshelf configuration
gotify_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
gotify_owner: "{{ ansible_user_id }}"
gotify_conf_dir: ~/conf/gotify
gotify_service_name: gotify
gotify_hostname: "{{ gotify_service_name }}"
gotify_vhost: "{{ gotify_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
gotify_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
gotify_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Gotify and tailscale.
gotify_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/gotify.env.j2') }}"
  # References:
  #   * https://gotify.net/docs/config#environment-variables
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # GOTIFY_REGISTRATION=false
# ----------
# The following settings configure Gotify to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
gotify_tailscaled: false
gotify_ts_version:           # Leave blank to leverage bookshelf configuration
gotify_ts_service_name: "{{ gotify_service_name }}-ts"
gotify_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
gotify_ts_hostname: "{{ gotify_service_name }}"
gotify_ts_auth_key:

# ====================
#       HEDGEDOC
# ====================
# VERSIONS: https://quay.io/repository/hedgedoc/hedgedoc?tab=tags
# -----
# NOTES:
#   * container is quite restricted. See restrictions in the main.env.j2 tempate.
#     They can be overriden with extra_env
#   * with restrictions in place users registration with ./bin/manage_users tool.
# -----
hedgedoc_managed: false   # <- LOW INTEREST, no testing
hedgedoc_enabled: true
hedgedoc_version:       # Leave blank to leverage bookshelf configuration
hedgedoc_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
hedgedoc_owner: "{{ ansible_user_id }}"
hedgedoc_conf_dir: ~/conf/hedgedoc
hedgedoc_service_name: hedgedoc
hedgedoc_hostname: "{{ hedgedoc_service_name }}"
hedgedoc_vhost: "{{ hedgedoc_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
hedgedoc_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
hedgedoc_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# HedgeDoc domain. Ex.: {{ hedgedoc_vhost }} or {{ ansible_default_ipv4.address }}
hedgedoc_domain: "{{ ansible_default_ipv4.address }}"       # (REQUIRED)
# Optional extra env vars
hedgedoc_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/hedgedoc.env.j2') }}"
  # References:
  #   * https://docs.hedgedoc.org/guides/reverse-proxy/#hedgedoc-config
  # ----------
  # CMD_URL_ADDPORT=false
  # CMD_PROTOCOL_USESSL=true
  # CMD_ALLOW_EMAIL_REGISTER=true   # <- Allow users registration

# ====================
#       HEIMDALL
# ====================
# VERSIONS: https://hub.docker.com/r/linuxserver/heimdall/tags
# -----
heimdall_managed: "{{ factum_os_family in ['alpine'] }}"
heimdall_enabled: true
heimdall_version:       # Leave blank to leverage bookshelf configuration
heimdall_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
heimdall_owner: "{{ ansible_user_id }}"
heimdall_conf_dir: ~/conf/heimdall
heimdall_service_name: heimdall
heimdall_hostname: "{{ heimdall_service_name }}"
heimdall_vhost: "{{ heimdall_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
heimdall_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
heimdall_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Ex.:
#   # heimdall_web_ui_port_ext - contains actual external port
#   http://{{ ansible_default_ipv4.address }}:{{ heimdall_web_ui_port_ext }}
#   https://{{ heimdall_vhost }}
heimdall_base_url: http://{{ ansible_default_ipv4.address }}:{{ heimdall_web_ui_port_ext }}     # (REQUIRED)

# ====================
#       JELLYFIN
# ====================
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/jellyfin/tags
#   * => See tailscale role versions
# -----
jellyfin_managed: "{{ factum_os_family in ['alpine'] }}"
jellyfin_enabled: true
jellyfin_version:       # Leave blank to leverage bookshelf configuration
jellyfin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
jellyfin_owner: bug2                      # <- Who runs the container
jellyfin_user: "{{ ansible_user_id }}"    # <- The in-container user and data filesystem owner
jellyfin_conf_dir: ~/conf/jellyfin
jellyfin_data_volumes:    # <- Volumes for data
  - ~/data/Movies:/data   # (REQUIRED) At least one volume required
  - ~/data/Parents/Movies:/extra/Parents
  - ~/data/Children/Movies:/extra/Children
jellyfin_service_name: jellyfin
jellyfin_hostname: "{{ jellyfin_service_name }}"
jellyfin_vhost: "{{ jellyfin_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
jellyfin_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
jellyfin_discovery_ports_exposed: true
jellyfin_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both jellyfin and tailscale.
jellyfin_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/jellyfin.env.j2') }}"
  # References:
  #   * https://hub.docker.com/r/linuxserver/jellyfin#parameters
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # JELLYFIN_PublishedServerUrl=https://{{ jellyfin_vhost }}
# ----------
# The following settings configure Jellyfin to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jellyfin_tailscaled: false
jellyfin_ts_version:            # Leave blank to leverage bookshelf configuration
jellyfin_ts_service_name: "{{ jellyfin_service_name }}-ts"
jellyfin_ts_funnelled: false    # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
jellyfin_ts_hostname: "{{ jellyfin_service_name }}"
jellyfin_ts_auth_key:

# =========================
#       JOPLIN-SERVER
# =========================
# VERSIONS:
#   * https://hub.docker.com/r/joplin/server/tags
#   * https://hub.docker.com/_/postgres/tags?name=alpine
# -----
joplin_server_managed: false    # <- LOW INTEREST, no testing
joplin_server_enabled: true
joplin_server_version:              # Leave blank to leverage bookshelf configuration
joplin_server_postgres_version:     # Leave blank to leverage bookshelf configuration
joplin_server_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
joplin_server_owner: "{{ ansible_user_id }}"
joplin_server_conf_dir: ~/conf/joplin-server
joplin_server_service_name: joplin-server
joplin_server_hostname: "{{ joplin_server_service_name }}"
joplin_server_vhost: "{{ joplin_server_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
joplin_server_web_ui_port:          # Leave blank for bookshelf default, or '-1' to not expose the port
# Joplin base url. Ex.:
#   # joplin_server_web_ui_port_ext - contains actual external port
#   https://{{ joplin_server_vhost }}
#   http://{{ ansible_default_ipv4.address }}:{{ joplin_server_web_ui_port_ext }}
joplin_server_base_url: http://{{ ansible_default_ipv4.address }}:{{ joplin_server_web_ui_port_ext }}
joplin_server_db_service_name: "{{ joplin_server_service_name }}-db"
joplin_server_db_pass: changeme
joplin_server_tz: "{{ ansible_date_time.tz | default('UTC') }}"

# =================
#       JOTTY
# =================
# VERSIONS:
#   * https://github.com/fccview/jotty/pkgs/container/jotty
#   * => See tailscale role versions
# -----
jotty_managed: "{{ factum_os_family in ['alpine'] }}"
jotty_enabled: true
jotty_version:            # Leave blank to leverage bookshelf configuration
jotty_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
jotty_owner: "{{ ansible_user_id }}"
jotty_conf_dir: ~/conf/jotty
jotty_service_name: jotty
jotty_hostname: "{{ jotty_service_name }}"
jotty_vhost: "{{ jotty_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
jotty_web_ui_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Jotty and Tailscale.
jotty_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/jotty.env.j2') }}"
  # References:
  #   * https://github.com/fccview/jotty/blob/main/howto/DOCKER.md#environment-variables
  # ----------
  # SERVE_PUBLIC_IMAGES=yes
# ----------
# The following settings configure Jotty to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
jotty_tailscaled: false
jotty_ts_version:          # Leave blank to leverage bookshelf configuration
jotty_ts_service_name: "{{ jotty_service_name }}-ts"
jotty_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
jotty_ts_hostname: "{{ jotty_service_name }}"
jotty_ts_auth_key:

# ==================
#       MEALIE
# ==================
# -----
# VERSIONS:
#   * https://github.com/mealie-recipes/mealie/pkgs/container/mealie
#   * => See tailscale role versions
# -----
mealie_managed: "{{ factum_os_family in ['alpine'] }}"
mealie_enabled: true
mealie_version:       # Leave blank to leverage bookshelf configuration
mealie_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
mealie_owner: "{{ ansible_user_id }}"
mealie_conf_dir: ~/conf/mealie
mealie_service_name: mealie
mealie_hostname: "{{ mealie_service_name }}"
mealie_vhost: "{{ mealie_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
mealie_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
mealie_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both Mealie and tailscale.
mealie_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/mealie.env.j2') }}"
  # References:
  #   * https://docs.mealie.io/documentation/getting-started/installation/backend-config/
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  ALLOW_SIGNUP=false
# ----------
# The following settings configure Mealie to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
mealie_tailscaled: false
mealie_ts_version:          # Leave blank to leverage bookshelf configuration
mealie_ts_service_name: "{{ mealie_service_name }}-ts"
mealie_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
mealie_ts_hostname: "{{ mealie_service_name }}"
mealie_ts_auth_key:

# =================
#       MEMOS
# =================
# VERSIONS:
#   * https://hub.docker.com/r/neosmemo/memos/tags
# -----
memos_managed: "{{ factum_os_family in ['alpine'] }}"
memos_enabled: true
memos_version:        # Leave blank to leverage bookshelf configuration
memos_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
memos_owner: "{{ ansible_user_id }}"
memos_conf_dir: ~/conf/memos
memos_service_name: memos
memos_hostname: "{{ memos_service_name }}"
memos_vhost: "{{ memos_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
memos_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Memos and tailscale.
memos_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/memos.env.j2') }}"
  # References:
  #   * https://www.usememos.com/docs/configuration
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # MEMOS_MAX_LOGIN_ATTEMPTS=5
  # MEMOS_LOCKOUT_DURATION=900
# ----------
# The following settings configure Memos to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
memos_tailscaled: false
memos_ts_version:           # Leave blank to leverage bookshelf configuration
memos_ts_service_name: "{{ memos_service_name }}-ts"
memos_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
memos_ts_hostname: "{{ memos_service_name }}"
memos_ts_auth_key:

# ==================
#       METUBE
# ==================
# VERSIONS: https://github.com/alexta69/metube/pkgs/container/metube
# -----
metube_managed: "{{ factum_os_family in ['alpine'] }}"
metube_enabled: true
metube_version:       # Leave blank to leverage bookshelf configuration
metube_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
metube_owner: bug2       # <- Who runs the container
metube_user: "{{ ansible_user_id }}"   # <- The in-container user and data filesystem owner
metube_conf_dir: ~/conf/metube
metube_data_dir: ~/data/Metube
metube_service_name: metube
metube_hostname: "{{ metube_service_name }}"
metube_vhost: "{{ metube_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
metube_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
metube_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars
metube_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/metube.env.j2') }}"
  # References:
  #   * https://github.com/alexta69/metube#%EF%B8%8F-configuration-via-environment-variables
  # ----------
  DEFAULT_THEME=dark

# =====================
#       NAVIDROME
# =====================
# VERSIONS: https://hub.docker.com/r/deluan/navidrome/tags
# -----
navidrome_managed: "{{ factum_os_family in ['alpine'] }}"
navidrome_enabled: true
navidrome_version:        # Leave blank to leverage bookshelf configuration
navidrome_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
navidrome_owner: bug2                       # <- Who runs the container
navidrome_user: "{{ ansible_user_id }}"     # <- The in-container user and data filesystem owner
navidrome_conf_dir: ~/conf/navidrome
navidrome_data_volumes:     # <- Volumes for data
  - ~/data/Music:/music:ro  # (REQUIRED) At least one volume required to be mounted to '/music'
  - ~/data/Metal:/extra/Metal:ro
  - ~/data/Pop:/extra/Pop:ro
navidrome_service_name: navidrome
navidrome_hostname: "{{ navidrome_service_name }}"
navidrome_vhost: "{{ navidrome_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
navidrome_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
navidrome_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both navidrome and tailscale.
navidrome_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/navidrome.env.j2') }}"
  # References:
  #   * https://www.navidrome.org/docs/usage/configuration-options/#available-options
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # ND_ENABLEINSIGHTSCOLLECTOR=true
# ----------
# The following settings configure Navidrome to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
navidrome_tailscaled: false
navidrome_ts_version:           # Leave blank to leverage bookshelf configuration
navidrome_ts_service_name: "{{ navidrome_service_name }}-ts"
navidrome_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
navidrome_ts_hostname: "{{ navidrome_service_name }}"
navidrome_ts_auth_key:

# =======================
#   NGINX-PROXY-MANAGER
# =======================
# VERSIONS: https://hub.docker.com/r/jc21/nginx-proxy-manager/tags
# -----
nginx_proxy_manager_managed: false
nginx_proxy_manager_enabled: true
nginx_proxy_manager_version:          # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_manager_owner: "{{ ansible_user_id }}"
nginx_proxy_manager_conf_dir: ~/conf/nginx-proxy-manager
nginx_proxy_manager_service_name: nginx-proxy-manager
nginx_proxy_manager_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_host_net: false   # 'true' allows adding Stream ports without editting compose file
# Port settings only get applied with nginx_proxy_manager_host_net == false
nginx_proxy_manager_web_ui_port:      # Leave blank for bookshelf default, or '-1' to not expose the port
nginx_proxy_manager_http_port:        # Leave blank for bookshelf default, or '-1' to not expose the port
nginx_proxy_manager_https_port:       # Leave blank for bookshelf default, or '-1' to not expose the port
nginx_proxy_manager_vhost: "{{ nginx_proxy_manager_service_name }}.domain.local"
nginx_proxy_manager_extra_env: |      # "{{ lookup('template', playbook_dir + '/resources/nginx-proxy-manager.env.j2') }}"
  FOO1=bar1
  FOO2=bar2
# ----------
# The following settings configure Nginx-Proxy-Manager to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
nginx_proxy_manager_tailscaled: false
nginx_proxy_manager_ts_version:           # Leave blank to leverage bookshelf configuration
nginx_proxy_manager_ts_service_name: "{{ nginx_proxy_manager_service_name }}-ts"
nginx_proxy_manager_ts_funnelled: false   # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
nginx_proxy_manager_ts_hostname: "{{ nginx_proxy_manager_service_name }}"
nginx_proxy_manager_ts_auth_key:

# ===============
#   NGINX-PROXY
# ===============
# Versions:
# * https://hub.docker.com/r/jwilder/nginx-proxy/tags?name=alpine
# Exposes:
# * nxinx_proxy_net - proxy network name
# -----
nginx_proxy_managed: "{{ factum_os_family in ['alpine'] }}"
nginx_proxy_enabled: true
nginx_proxy_version:        # Leave blank to leverage bookshelf configuration
nginx_proxy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
nginx_proxy_owner: "{{ ansible_user_id }}"
nginx_proxy_service_name: nginx-proxy
nginx_proxy_hostname: "{{ nginx_proxy_service_name }}"
nginx_proxy_http_port: "{{ service_port.nginx_proxy_http }}"     # Ex.: '80', no HTTP when empty
nginx_proxy_https_port:     # Ex.: '443', no HTTPS when empty
nginx_proxy_certs_dir:      # Path to certs on the managed machine. Works paired with HTTPS

# ====================
#       OLIVETIN
# ====================
olivetin_managed: "{{ factum_os_family in ['alpine'] }}"
olivetin_enabled: true
olivetin_version:       # Leave blank to leverage bookshelf configuration
olivetin_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
olivetin_owner: "{{ ansible_user_id }}"
olivetin_conf_dir: ~/conf/olivetin
olivetin_service_name: olivetin
olivetin_hostname: "{{ olivetin_service_name }}"
olivetin_vhost: "{{ olivetin_service_name }}.domain.local"
olivetin_web_ui_port:   # Leave blank to leverage bookshelf configuration
olivetin_scripts:
  - text: "{{ lookup('file', 'sample.sh') }}"
    name: sample.sh
olivetin_volumes:
  - '~:/app/home:ro'
olivetin_config: "{{ lookup('file', 'config.yaml') }}"

# ====================
#       PAIRDROP
# ====================
# Transfer Files Cross-Platform. No Setup, No Signup.
#   https://github.com/schlagmichdoch/PairDrop
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/pairdrop/tags?name=-ls
#   * => See tailscale role versions
# -----
pairdrop_managed: "{{ factum_os_family in ['alpine'] }}"
pairdrop_enabled: true
pairdrop_version:       # Leave blank to leverage bookshelf configuration
pairdrop_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
pairdrop_owner: "{{ ansible_user_id }}"
pairdrop_service_name: pairdrop
pairdrop_hostname: "{{ pairdrop_service_name }}"
pairdrop_vhost: "{{ pairdrop_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
pairdrop_web_ui_port:   # Leave blank for bookshelf default, or '-1' to not expose the port
pairdrop_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars, will affect both PairDrop and tailscale.
pairdrop_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/pairdrop.env.j2') }}"
  # References:
  #   * https://hub.docker.com/r/linuxserver/pairdrop#parameters
  #   * https://tailscale.com/kb/1282/docker#parameters
  # ----------
  # RATE_LIMIT=true
# ----------
# The following settings configure PairDrop to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
pairdrop_ts_version:          # Leave blank to leverage bookshelf configuration
pairdrop_ts_service_name: "{{ pairdrop_service_name }}-ts"
pairdrop_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
pairdrop_ts_hostname: "{{ pairdrop_service_name }}"
pairdrop_ts_auth_key:

# ===========================
#       PORTAINER-AGENT
# ===========================
# Versions:
# * https://hub.docker.com/r/portainer/agent/tags?name=alpine
# -----
portainer_agent_managed: "{{ factum_os_family in ['alpine'] }}"
portainer_agent_enabled: true
portainer_agent_version:        # Leave blank to leverage bookshelf configuration
portainer_agent_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_agent_conf_dir: ~/conf/portainer-agent
portainer_agent_owner: "{{ ansible_user_id }}"
portainer_agent_service_name: portainer-agent
portainer_agent_hostname: "{{ portainer_agent_service_name }}"
portainer_agent_host_management: true
# Can disable ports if plan to access only via tailnet or reverse-proxy
portainer_agent_tcp_port: 9002  # Leave blank for bookshelf default, or '-1' to not expose the port
# Optional extra env vars, will affect both Portainer agent and Tailscale.
portainer_agent_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/portainer-agent.env.j2') }}"
# ----------
# The following settings configure Portainer agent to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
portainer_agent_tailscaled: false
portainer_agent_ts_version:     # Leave blank to leverage bookshelf configuration
portainer_agent_ts_service_name: "{{ portainer_agent_service_name }}-ts"
# Hostname within tailnet
portainer_agent_ts_hostname: "{{ portainer_agent_service_name }}"
portainer_agent_ts_auth_key:

# =====================
#       PORTAINER
# =====================
# Versions:
# * https://hub.docker.com/r/portainer/portainer-ce/tags?name=alpine
# -----
portainer_managed: "{{ factum_os_family in ['alpine'] }}"
portainer_enabled: true
portainer_version:        # Leave blank to leverage bookshelf configuration
portainer_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
portainer_owner: "{{ ansible_user_id }}"
portainer_conf_dir: ~/conf/portainer
portainer_service_name: portainer
portainer_hostname: "{{ portainer_service_name }}"
portainer_vhost: "{{ portainer_service_name }}.domain.local"
# Can disable ports if plan to access only via tailnet or reverse-proxy
portainer_web_ui_port:    # Leave blank for bookshelf default, or '-1' to not expose the port
# Expose port for agents
portainer_expose_ssh_tunnel_port: false
# Optional extra env vars, will affect both Portainer and Tailscale.
portainer_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/portainer.env.j2') }}"
# ----------
# The following settings configure Portainer to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
portainer_tailscaled: false
portainer_ts_version:          # Leave blank to leverage bookshelf configuration
portainer_ts_service_name: "{{ portainer_service_name }}-ts"
portainer_ts_funnelled: false  # Expose to the world via TS funnel. Use with caution
# Hostname within tailnet
portainer_ts_hostname: "{{ portainer_service_name }}"
portainer_ts_auth_key:

# =======================
#       QBITTORRENT
# =======================
# Free and reliable P2P Bittorrent client
#   https://www.qbittorrent.org/
#   https://hub.docker.com/r/linuxserver/qbittorrent
# -----
# Versions:
# * https://hub.docker.com/r/linuxserver/qbittorrent/tags?name=-ls
# * https://github.com/VueTorrent/VueTorrent/pkgs/container/vuetorrent-lsio-mod/versions
# -----
# NOTE 1: Temporary password for the admin user will be printed to the container
# log on startup
#
# NOTE 2: Switch to alternative UI
#   Settings > WebUI > check Use alternative WebUI, Files location: /vuetorrent
qbittorrent_managed: "{{ factum_os_family in ['alpine'] }}"
qbittorrent_enabled: true
qbittorrent_version:              # Leave blank to leverage bookshelf configuration
qbittorrent_vuetorrent_version:   # Leave blank to leverage bookshelf configuration
qbittorrent_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
qbittorrent_owner: bug2                     # <- Who runs the container
qbittorrent_user: "{{ ansible_user_id }}"   # <- The in-container user and data filesystem owner
qbittorrent_conf_dir: ~/conf/qbittorrent
qbittorrent_data_volumes:       # <- Volumes for data
  - ~/data/Torrent:/downloads   # (REQUIRED) At least one volume required to be mounted to '/downloads'
  - ~/data/Movies/Torrent:/extra/Movies
  - ~/data/Music/Torrent:/extra/Music
qbittorrent_service_name: qbittorrent
qbittorrent_hostname: "{{ qbittorrent_service_name }}"
qbittorrent_vhost: "{{ qbittorrent_service_name }}.domain.local"
qbittorrent_web_ui_port:          # Leave blank to leverage bookshelf configuration
qbittorrent_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional. Configure notifications script, that will be available in the container
#   /scripts/notify.sh
# Configure in qBittorrent UI -> Settings -> Downloads -> External ... on finished
# Leave blank to skip configuration. If not blank, rest notify_* should be
# configured accordingly
qbittorrent_notify_provider: gotify             # discord|gotify|telegram
qbittorrent_notify_token: changeme              # "{{ lookup('file', playbook_dir + '/resources/qbittorrent.secret.sh') }}"
qbittorrent_notify_server: https://gotify.home  # (REQUIRED for gotify)
qbittorrent_notify_chat_id:                     # (REQUIRED for telegram)
# Optional extra env vars
qbittorrent_extra_env: |          # "{{ lookup('template', playbook_dir + '/resources/qbittorrent.env.j2') }}"
  FOO1=bar1
  FOO2=bar2

# =================
#       SAMBA
# =================
# Versions:
# * https://hub.docker.com/r/crazymax/samba/tags
# -----
samba_managed: "{{ factum_os_family in ['alpine'] }}"
samba_enabled: true
samba_version:        # Leave blank to leverage bookshelf configuration
samba_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
samba_owner: "{{ ansible_user_id }}"
samba_shares_dir: ~/Samba    # (REQUIRED) Shares root directory
samba_service_name: samba
samba_hostname: "{{ samba_service_name }}"
samba_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional path to extra configuration .env.j2 file
# Ex.: "{{ playbook_dir }}/resources/samba/extra.env.j2"
samba_extra_envfile:

# ========================
#       SILVERBULLET
# ========================
# An open source personal productivity platform built on Markdown, turbo
# charged with the scripting power of Lua
#   https://silverbullet.md/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/zefhemel/silverbullet/tags
# -----
silverbullet_managed: "{{ factum_os_family in ['alpine'] }}"
silverbullet_enabled: true
silverbullet_version:       # Leave blank to leverage bookshelf configuration
silverbullet_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
silverbullet_owner: "{{ ansible_user_id }}"   # <- Who runs the container
silverbullet_conf_dir: ~/conf/silverbullet
silverbullet_service_name: silverbullet
silverbullet_hostname: "{{ silverbullet_service_name }}"
silverbullet_vhost: "{{ silverbullet_service_name }}.domain.local"
silverbullet_web_ui_port:   # Leave blank to leverage bookshelf configuration
# Auth
silverbullet_auth_user: admin    # (OPTIONAL)
silverbullet_auth_pass: changeme # (REQUIRED when non-empty user) If user not set, user defaults to 'admin'
# silverbullet_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional extra env vars
silverbullet_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/silverbullet.env.j2') }}"
  FOO1=bar1
  FOO2=bar2

# ====================
#       SMTP4DEV
# ====================
# VERSIONS:
#   * https://hub.docker.com/r/rnwood/smtp4dev/tags?name=v
#   * => See tailscale role versions
# -----
smtp4dev_managed: "{{ factum_os_family in ['alpine'] }}"
smtp4dev_enabled: true
smtp4dev_version:       # Leave blank to leverage bookshelf configuration
smtp4dev_compose_dir:   # Unless you know ..., leave blank to leverage bookshelf configuration
smtp4dev_owner: "{{ ansible_user_id }}"
smtp4dev_conf_dir: ~/conf/smtp4dev
smtp4dev_service_name: smtp4dev
smtp4dev_hostname: "{{ smtp4dev_service_name }}"
smtp4dev_vhost: "{{ smtp4dev_service_name }}.domain.local"
smtp4dev_web_ui_port:   # Leave blank to leverage bookshelf configuration
smtp4dev_smtp_port:     # Leave blank to leverage bookshelf configuration
smtp4dev_imap_port:     # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both smtp4dev and tailscale.
smtp4dev_extra_env: |   # "{{ lookup('template', playbook_dir + '/resources/smtp4dev.env.j2') }}"
  FOO1=bar1
  FOO2=bar2
# ----------
# The following settings configure smtp4dev to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
smtp4dev_tailscaled: false
smtp4dev_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
smtp4dev_tailscale_ts_hostname: "{{ smtp4dev_service_name }}-tailscale"
smtp4dev_tailscale_auth_key:

# =====================
#       SYNCTHING
# =====================
# Versions:
# * https://hub.docker.com/r/linuxserver/syncthing/tags?name=-ls
# -----
syncthing_managed: "{{ factum_os_family in ['alpine'] }}"
syncthing_enabled: true
syncthing_version:              # Leave blank to leverage bookshelf configuration
syncthing_compose_dir:          # Unless you know ..., leave blank to leverage bookshelf configuration
syncthing_owner: bug2  # <- Who runs the container
syncthing_user: "{{ ansible_user_id }}"   # <- The in-container user and data filesystem owner
syncthing_conf_dir: ~/conf/syncthing
syncthing_data_volumes:   # <- Volumes for data
  - ~/data:/data          # (REQUIRED) At least one volume required
  - ~/data2:/extra/data2
  - ~/data3:/extra/data3
syncthing_service_name: syncthing
syncthing_hostname: "{{ syncthing_service_name }}"
syncthing_vhost: "{{ syncthing_service_name }}.domain.local"
syncthing_web_ui_port:          # Leave blank to leverage bookshelf configuration
syncthing_tz: "{{ ansible_date_time.tz | default('UTC') }}"
syncthing_extra_env: |          # "{{ lookup('template', playbook_dir + '/resources/syncthing.env.j2') }}"
  FOO1=bar1
  FOO2=bar2
# ----------
# The following settings configure Syncthing to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
syncthing_tailscaled: false
syncthing_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
syncthing_tailscale_ts_hostname: "{{ syncthing_service_name }}-tailscale"
syncthing_tailscale_auth_key:

# =====================
#       TAILSCALE
# =====================
# Versions:
# * https://hub.docker.com/r/tailscale/tailscale/tags
# -----
# IMPORTANT 1: Auth key is required for initial container run:
# https://tailscale.com/admin/settings/keys -> 'Generate auth key...'
#   -> generate single time or reusable
#
# IMPORTANT 2: After deployment https://tailscale.com/admin/machines -> 3 dots
# next to the machine -> Disable key expiry (to avoid re-login to tailscale)
#
# IMPORTANT 3: With changed routes or exit node params service requires a valid
# auth key, i.e. if it's one-timer new one must be provided
#
# NOTE 1: The service is useless unless it's configured for exit node or subnet
# router. Both need to be confirmed in the machines admin
tailscale_managed: false
tailscale_enabled: false
tailscale_version:          # Leave blank to leverage bookshelf configuration
tailscale_compose_dir:      # Unless you know ..., leave blank to leverage bookshelf configuration
tailscale_owner: "{{ ansible_user_id }}"
tailscale_conf_dir: ~/conf/tailscale
tailscale_service_name: tailscale
tailscale_hostname: "{{ tailscale_service_name }}"
tailscale_auth_key:
# Ex.: '192.168.0.0/24,192.168.2.0/24'
tailscale_routes:
tailscale_exit_node: false  # Advertise exit node
tailscale_tz: "{{ ansible_date_time.tz | default('UTC', true) }}"
# Optional extra env vars
tailscale_extra_env: |      # "{{ lookup('file', playbook_dir + '/resources/tailscale.env') }}"
  # These are used in demo gatus.sh healthcheck script.
  #   https://github.com/TwiN/gatus/#external-endpoints
  # ----------
  GATUS_URL=https://gatus.domain.local
  GATUS_KEY=bookshelf_tailscale   # {GROUP}_{ENDPOINT}
tailscale_secrets_env: |     # "{{ lookup('file', playbook_dir + '/resources/tailscale.secret.env') }}"
  # Same as extra env, but for secrets
  # ----------
  GATUS_TOKEN=changeme
# ----------
# The following settings configure healthcheck reporter sidecar container.
# The container will start if tailscale_health_script not empty.
# The container is equipped with curl, wget and bash.
tailscale_health_script:           # "{{ lookup('file', 'gatus.sh') }}"
tailscale_health_alpine_version:   # Leave blank to leverage bookshelf configuration
tailscale_health_service_name: "{{ tailscale_service_name }}-health"
tailscale_health_check_schedule: '*/1 * * * *'   # <- Every minute

# ======================
#       TORRSERVER
# ======================
# Torrent stream server
#   https://github.com/YouROK/TorrServer
# -----
# VERSIONS:
#   * https://github.com/yourok/TorrServer/pkgs/container/torrserver
#   * => See tailscale role versions
# -----
torrserver_managed: "{{ factum_os_family in ['alpine'] }}"
torrserver_enabled: true
torrserver_version:            # Leave blank to leverage bookshelf configuration
torrserver_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
torrserver_owner: "{{ ansible_user_id }}"
torrserver_conf_dir: ~/conf/torrserver
torrserver_service_name: torrserver
torrserver_hostname: "{{ torrserver_service_name }}"
torrserver_vhost: "{{ torrserver_service_name }}.domain.local"
torrserver_web_ui_port:        # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both TorrServer and Tailscale.
torrserver_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/torrserver.env.j2') }}"
  # References:
  #   * https://github.com/YouROK/TorrServer/#environments
  # ----------
  # Auth seems to break the server
  # TS_HTTPAUTH=1   # <- Enable auth. https://github.com/YouROK/TorrServer/#authentication
# ----------
# Disable if plan to access only via tailnet or reverse-proxy
torrserver_web_ui_ports_exposed: true
# ----------
# The following settings configure TorrServer to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
torrserver_tailscaled: false
torrserver_ts_version:   # Leave blank to leverage bookshelf configuration
torrserver_ts_service_name: "{{ torrserver_service_name }}-ts"
# Hostname within tailnet
torrserver_ts_hostname: "{{ torrserver_service_name }}"
torrserver_ts_auth_key:

# ====================
#       TWINGATE
# ====================
# Versions:
# * https://hub.docker.com/r/twingate/connector/tags
# -----
twingate_managed: false
twingate_enabled: false
twingate_version:         # Leave blank to leverage bookshelf configuration
twingate_compose_dir:     # Unless you know ..., leave blank to leverage bookshelf configuration
twingate_owner: "{{ ansible_user_id }}"
twingate_service_name: twingate
twingate_hostname: "{{ twingate_service_name }}"
twingate_tenant_name:     # Your twingate network name. Required
# https://<tenant>.twingate.com/networks/overview -> 'Deploy ... Connector'
#   -> 'Docker' -> 'Generate Tokens'
twingate_access_token:    # Required
twingate_refresh_token:   # Required
twingate_extra_env: |     # "{{ lookup('file', playbook_dir + '/resources/twingate.env') }}"
  FOO1=bar1
  FOO2=bar2

# =======================
#       UPTIME-KUMA
# =======================
# VERSIONS:
#   * https://hub.docker.com/r/louislam/uptime-kuma/tags
#   * => See tailscale role versions
# -----
uptime_kuma_managed: false  # <- LOW INTEREST, no testing
uptime_kuma_enabled: true
uptime_kuma_version:        # Leave blank to leverage bookshelf configuration
uptime_kuma_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
uptime_kuma_owner: "{{ ansible_user_id }}"
uptime_kuma_conf_dir: ~/conf/uptime-kuma
uptime_kuma_service_name: uptime-kuma
uptime_kuma_hostname: "{{ uptime_kuma_service_name }}"
uptime_kuma_vhost: "{{ uptime_kuma_service_name }}.domain.local"
uptime_kuma_web_ui_port:    # Leave blank to leverage bookshelf configuration
# Optional extra env vars, will affect both Uptime Kuma and tailscale.
uptime_kuma_extra_env: |    # "{{ lookup('template', playbook_dir + '/resources/uptime-kuma.env.j2') }}"
  FOO1=bar1
  FOO2=bar2
# ----------
# The following settings configure Uptime Kuma to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
uptime_kuma_tailscaled: false
uptime_kuma_tailscale_version:    # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
uptime_kuma_tailscale_ts_hostname: "{{ uptime_kuma_service_name }}-tailscale"
uptime_kuma_tailscale_auth_key:

# =======================
#       VAULTWARDEN
# =======================
# The community driven password manager.
#   https://www.vaultwarden.net/
# -----
# Versions:
# * https://hub.docker.com/r/vaultwarden/server/tags?name=alpine
# Options description:
# * https://github.com/dani-garcia/vaultwarden/blob/main/.env.template
# -----
vaultwarden_managed: "{{ factum_os_family in ['alpine'] }}"
vaultwarden_enabled: true
vaultwarden_version:        # Leave blank to leverage bookshelf configuration
vaultwarden_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
vaultwarden_owner: "{{ ansible_user_id }}"
vaultwarden_conf_dir: ~/conf/vaultwarden
vaultwarden_service_name: vaultwarden
vaultwarden_hostname: "{{ vaultwarden_service_name }}"
vaultwarden_vhost: "{{ vaultwarden_service_name }}.domain.local"
# Required with https via reverse proxy, vaultwarden needs to know it's https
# to work properly with attachments.
# https://github.com/dani-garcia/vaultwarden/wiki/Using-Docker-Compose
# Ex.: https://{{ vaultwarden_vhost }}
vaultwarden_vdomain: https://{{ vaultwarden_vhost }}
# Access token for http(s)://VAULTWARDEN_HOST/admin
# Generate with:
#   docker container run -it --rm vaultwarden/server:latest /vaultwarden hash
vaultwarden_admin_token_hash:
vaultwarden_web_ui_port:    # Leave blank to leverage bookshelf configuration
vaultwarden_tz: "{{ ansible_date_time.tz | default('UTC') }}"
vaultwarden_extra_env: |    # "{{ lookup('file', playbook_dir + '/resources/vaultwarden.env') }}"
  FOO1=bar1
  FOO2=bar2

# ===================
#       VIKUNJA
# ===================
vikunja_managed: "{{ factum_os_family in ['alpine'] }}"
vikunja_enabled: true
vikunja_version:            # Leave blank to leverage bookshelf configuration
vikunja_postgres_version:   # Leave blank to leverage bookshelf configuration
vikunja_compose_dir:        # Unless you know ..., leave blank to leverage bookshelf configuration
vikunja_owner: "{{ ansible_user_id }}"
vikunja_conf_dir: ~/conf/vikunja
vikunja_service_name: vikunja
vikunja_hostname: "{{ vikunja_service_name }}"
vikunja_vhost: "{{ vikunja_service_name }}.domain.local"
vikunja_web_ui_port:        # Leave blank to leverage bookshelf configuration
vikunja_tz: "{{ ansible_date_time.tz | default('UTC') }}"
vikunja_db_pass: changeme     # (REQUIRED) Something hard and random
vikunja_jwtsecret: changeme   # (REQUIRED) Something hard and random
# Optional extra env vars, will affect both Vikunja and Tailscale.
vikunja_extra_env: |        # "{{ lookup('template', playbook_dir + '/resources/vikunja.env.j2') }}"
  FOO1=bar1
  FOO2=bar2
# ----------
# IMPORTANT 1: See IMPORTANTs in tailscale service description
vikunja_tailscaled: false
vikunja_tailscale_version:  # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
vikunja_tailscale_ts_hostname: "{{ vikunja_service_name }}-tailscale"
vikunja_tailscale_auth_key:

# ===================
#       WG-EASY
# ===================
# The easiest way to run WireGuard VPN + Web-based Admin UI.
#   https://github.com/wg-easy/wg-easy
# -----
# Versions:
# * https://github.com/wg-easy/wg-easy/pkgs/container/wg-easy/versions?filters%5Bversion_type%5D=tagged
# Options description:
# * https://github.com/wg-easy/wg-easy/tree/production?tab=readme-ov-file#options
# * https://github.com/wg-easy/wg-easy/blob/production/How_to_generate_an_bcrypt_hash.md
# -----
wg_easy_managed: "{{ factum_os_family in ['alpine'] }}"
wg_easy_enabled: true
wg_easy_version:        # Leave blank to leverage bookshelf configuration
wg_easy_compose_dir:    # Unless you know ..., leave blank to leverage bookshelf configuration
wg_easy_owner: "{{ ansible_user_id }}"
wg_easy_conf_dir: ~/conf/wg-easy
wg_easy_service_name: wg-easy
wg_easy_hostname: "{{ wg_easy_service_name }}"
wg_easy_vhost: "{{ wg_easy_service_name }}.domain.local"
wg_easy_web_ui_port:    # Leave blank to leverage bookshelf configuration
wg_easy_udp_port: 51820 # Must match the UDP port configured in UI
wg_easy_insecure: 'true'  # Allow login via plain http
# Optional extra env vars
wg_easy_extra_env:      # "{{ lookup('template', playbook_dir + '/resources/wg-easy.env.j2') }}"

# ==================
#       WIKIJS
# ==================
# A modern and powerful wiki app built on Node.js
#   https://js.wiki/
# -----
# VERSIONS:
#   * https://hub.docker.com/r/linuxserver/wikijs/tags?name=-ls
#   * https://hub.docker.com/_/postgres/tags?name=alpine
#   * => See tailscale role versions
# -----
wikijs_managed: "{{ factum_os_family in ['alpine'] }}"
wikijs_enabled: true
wikijs_version:             # Leave blank to leverage bookshelf configuration
wikijs_postgres_version:    # Leave blank to leverage bookshelf configuration
wikijs_compose_dir:         # Unless you know ..., leave blank to leverage bookshelf configuration
wikijs_owner: "{{ ansible_user_id }}"
wikijs_conf_dir: ~/conf/wikijs
wikijs_service_name: wikijs
wikijs_hostname: "{{ wikijs_service_name }}"
wikijs_vhost: "{{ wikijs_service_name }}.domain.local"
wikijs_web_ui_port:         # Leave blank to leverage bookshelf configuration
wikijs_db_pass: changeme    # (REQUIRED) Something hard and random
wikijs_tz: "{{ ansible_date_time.tz | default('UTC') }}"
# Optional extra env vars, will affect both Wiki.js and tailscale.
wikijs_extra_env:           # "{{ lookup('template', playbook_dir + '/resources/wikijs.env.j2') }}"
# ----------
# The following settings configure Wiki.js to run as a part of tailnet
#
# IMPORTANT 1: See IMPORTANTs in tailscale service description
wikijs_tailscaled: false
wikijs_tailscale_version:   # Leave blank to leverage bookshelf configuration
# Hostname within tailnet
wikijs_tailscale_ts_hostname: "{{ wikijs_service_name }}-tailscale"
wikijs_tailscale_auth_key:



###################
###   DESKTOP   ###
###################

# ============
#   AUDACITY
# ============
audacity_managed: false

# =================
#       BRAVE
# =================
brave_managed: false

# ==================
#       CHROME
# ==================
chrome_managed: false

# =================
#       COPYQ
# =================
copyq_managed: false
# 'pm' or 'flatpak'. With non-ubuntu always forced to 'flatpak'
copyq_install_method: pm
copyq_starters:         # <- Existing users to have copyq autostarted
  # - "{{ ansible_user_id }}"

# ===================
#       DBEAVER
# ===================
dbeaver_managed: false
# 'snap' or 'flatpak'. With non-ubuntu always forced to 'flatpak'
dbeaver_install_method: flatpak

# =====================
#       DOUBLECMD
# =====================
# Cross platform open source file manager with two panels side by side.
#   https://doublecmd.sourceforge.io/
# -----
doublecmd_managed: false    # "{{ factum_os_family in ['debian'] }}"  # <- Only supported by Debian family
doublecmd_ui_lib: gtk       # <- 'gtk' for Gnome or 'qt' for KDE
doublecmd_starters:         # <- Existing users to have doublecmd autostarted
  # - "{{ ansible_user_id }}"

# ===========
#   FLATPAK
# ===========
flatpak_managed: false
flatpak_repos:    # <- Currently only the ones in the example are supported
  # - flathub

# ================
#       GIMP
# ================
gimp_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
gimp_install_method: flatpak

# ======================
#       INPUT-LEAP
# ======================
input_leap_managed: false
input_leap_starters:  # <- Existing users to have InputLeap autostarted
  # - "{{ ansible_user_id }}"

# =========================
#       INTELLIJ-IDEA
# =========================
intellij_idea_managed: false
# Ultimate edition, unless community
intellij_idea_ultimate: false

# ===========================
#       JELLYFIN-PLAYER
# ===========================
# Desktop client for Jellyfin media server.
#   https://github.com/jellyfin/jellyfin-media-player
# -----
jellyfin_player_managed: false
jellyfin_player_starters:   # <- Existing users to have jellyfin player autostarted
  # - "{{ ansible_user_id }}"

# ==========================
#       JOPLIN-DESKTOP
# ==========================
joplin_desktop_managed: false
# Supported values in prefered order (due newer versions in snap):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
joplin_desktop_install_method: snap

# ====================
#       KDENLIVE
# ====================
kdenlive_managed: false
# 'snap' or 'flatpak'
kdenlive_install_method: snap

# ======================
#       MKVTOOLNIX
# ======================
mkvtoolnix_managed: false

# ====================
#       NXPLAYER
# ====================
nxplayer_managed: false

# ======================
#       OBS-STUDIO
# ======================
obs_studio_managed: false

# =================
#       OPERA
# =================
opera_managed: false

# ============
#   PHPSTORM
# ============
phpstorm_managed: false

# =================
#       PINTA
# =================
pinta_managed: false
# 'snap' or 'flatpak'
pinta_install_method: flatpak

# ===================
#       POSTMAN
# ===================
postman_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
postman_install_method: flatpak

# ===========
#   SUBLIME
# ===========
sublime_managed: false
sublime_merge: false
# Supported values in prefered order (due to available CLI interface):
# * pm    # <- Forced for non-Ubuntu-like
# * snap
sublime_install_method: pm

# =========
#   TEAMS
# =========
teams_managed: false
teams_starters:   # <- Existing users to have teams autostarted
  # - "{{ ansible_user_id }}"

# ====================
#       TELEGRAM
# ====================
telegram_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
telegram_install_method: flatpak
telegram_starters:         # <- Existing users to have telegram autostarted
  # - "{{ ansible_user_id }}"

# ======================
#       TERMINATOR
# ======================
terminator_managed: false

# =========
#   VIBER
# =========
viber_managed: false
viber_starters:   # <- Existing users to have viber autostarted
  # - "{{ ansible_user_id }}"

# =======
#   VLC
# =======
vlc_managed: false

# ==========
#   VSCODE
# ==========
vscode_managed: false
# Supported values in prefered order (due to available CLI interface):
# * snap
# * flatpak   # <- Forced for non-Ubuntu-like
vscode_install_method: snap
vscode_extensions:      # <- Per user extensions
  # - owner: "{{ ansible_user_id }}"  # <- Existing user. Must be unique
  #   extensions:
  #     - editorconfig.editorconfig
  #     - mads-hartmann.bash-ide-vscode
  #     - timonwong.shellcheck
  #     - foxundermoon.shell-format
  #     - redhat.ansible

# ===================
#       WHATSIE
# ===================
whatsie_managed: false
# Supported values in prefered order (due to GUI oriented usage):
# * flatpak   # <- Forced for non-Ubuntu-like
# * snap
whatsie_install_method: flatpak



#######################
###       DEV       ###
#######################

# ==================
#       GOLANG
# ==================
golang_managed: "{{ factum_os_like in ['alpine', 'ubuntu'] }}"  # <- Only supported

# ==================
#       NODEJS
# ==================
nodejs_managed: "{{ factum_os_like in ['alpine'] }}"  # "{{ factum_os_like in ['alpine', 'ubuntu'] }}"  # <- Only supported by these
# Versions (better pick latest LTS) and channels lists:
#   * https://nodejs.org/en/about/previous-releases#looking-for-the-latest-release-of-a-version-branch
#   * https://snapcraft.io/node
nodejs_snap_channel:    # Only applicable for Ubuntu. Leave blank to leverage bookshelf defaults
